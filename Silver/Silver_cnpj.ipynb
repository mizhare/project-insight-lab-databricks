{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b630a8-41e7-4d26-a4ed-996a92cd5529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver - Filtros e Transformações CNPJ (Incremental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a8b3d4-32d1-4680-ac18-e12aa3f8cc65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Users/kgenuins@emeal.nttdata.com/project-insight-lab-databricks\")\n",
    "\n",
    "from Config.spark_config import apply_storage_config\n",
    "from Config.storage_config import *\n",
    "from Utils.utils import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, lit, upper, lower, regexp_replace, translate,\n",
    "    lpad, to_date, current_timestamp, decode, encode, concat_ws,\n",
    "    collect_list, explode, split, substring, xxhash64, percentile_approx, max, length,coalesce\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "import uuid\n",
    "import builtins\n",
    "from typing import Any\n",
    "\n",
    "apply_storage_config(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd4b18b-1e0b-4b89-9fe2-f13ec86847bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Localizações dos dados\n",
    "path_storage_bronze = f\"{bronze_path}\"\n",
    "path_storage_silver = f\"{silver_path}\"\n",
    "dbutils.fs.ls(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1bfd1d-5fbf-49bb-9e43-5d3594d88a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tabela de controle incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86fafe5-779f-40b3-aff7-1c5c066cc4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_tabela_controle_silver():\n",
    "    # Criar tabela de controle se não existir\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS silver.transformacoes_processadas_cnpj (\n",
    "            tabela_silver STRING,\n",
    "            data_ultima_transformacao TIMESTAMP,\n",
    "            total_registros BIGINT,\n",
    "            data_execucao TIMESTAMP\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "    print(\"Tabela de controle de transformações Silver criada!\")\n",
    "\n",
    "criar_tabela_controle_silver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0db591b-9822-4e65-a694-47662e5623c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções auxiliares para controle incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d37849-36d0-492e-920d-361db3ed0bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def obter_data_ultima_transformacao(tabela_silver):\n",
    "    \"\"\"\n",
    "    Obtém a data da última transformação realizada para uma tabela\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resultado = spark.sql(f\"\"\"\n",
    "            SELECT MAX(data_ultima_transformacao) as ultima_data\n",
    "            FROM silver.transformacoes_processadas_cnpj\n",
    "            WHERE tabela_silver = '{tabela_silver}'\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if resultado and resultado[0].ultima_data:\n",
    "            return resultado[0].ultima_data\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f353734-6534-434f-9518-f5cc07c4e61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def registrar_transformacao_silver(tabela_silver, total_registros):\n",
    "    \"\"\"\n",
    "    Registra transformação realizada na Silver\n",
    "    \"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO silver.transformacoes_processadas_cnpj\n",
    "        VALUES (\n",
    "            '{tabela_silver}',\n",
    "            current_timestamp(),\n",
    "            {total_registros},\n",
    "            current_timestamp()\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e26c31-c4e7-40f3-849c-26337ccecf37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções utilitárias do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee879c3-617c-4bda-b253-0eaf06f5ee10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def latin1_to_utf8(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\" Tenta converter colunas do tipo StringType de iso-8859-1(latin1) para UTF-8 \"\"\"\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(\n",
    "            column_name, \n",
    "            decode(encode(col(column_name), \"ISO-8859-1\"), \"UTF-8\")\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def strip_df(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\"Remove espaços em branco do início e do fim das strings\"\"\"\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(column_name, trim(column_name))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_accents(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\"Mapeia e remove acentos\"\"\"\n",
    "    accented = \"áàâãäéèêëíìîïóòôõöúùûüçÁÀÂÃÄÉÈÊËÍÌÎÏÓÒÔÕÖÚÙÛÜÇ\"\n",
    "    unaccented = \"aaaaaeeeeiiiiooooouuuucAAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(\n",
    "            column_name,\n",
    "            regexp_replace(\n",
    "                translate(col(column_name), accented, unaccented),\n",
    "                r\"[^a-zA-Z0-9\\s]\",\n",
    "                \"\"\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_dates(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\"Converte colunas para tipo DATE\"\"\"\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(column_name, to_date(col(column_name), \"yyyyMMdd\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_blanks_to_null(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza para NULL:\n",
    "      - valores já nulos (None/null)\n",
    "      - strings vazias (\"\")\n",
    "      - strings contendo apenas espaços (\"   \", \"\\t\", etc. após trim)\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        target_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "    else:\n",
    "        string_cols = {c for c, t in df.dtypes if t == \"string\"}\n",
    "        target_cols = [c for c in columns if c in string_cols]\n",
    "\n",
    "    for c in target_cols:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            when(col(c).isNull() | (trim(col(c)) == \"\"), None).otherwise(col(c))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "# def non_values_to_not_informed(df: DataFrame, columns: list[str], target_values: list[str], not_informed_str=\"NAO INFORMADO\") -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Converte para 'NAO INFORMADO':\n",
    "#       - valores contidos em 'target_values'\n",
    "#       - strings com valor \"0\"\n",
    "#       - strings vazias (\"\")\n",
    "#       - strings contendo apenas espaços\n",
    "#     \"\"\"\n",
    "#     if columns is None:\n",
    "#         target_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "#     else:\n",
    "#         string_cols = {c for c, t in df.dtypes if t == \"string\"}\n",
    "#         target_cols = [c for c in columns if c in string_cols]\n",
    "\n",
    "#     for c in target_cols:\n",
    "#         for target_value in target_values:\n",
    "#             df = df.withColumn(\n",
    "#                 c,\n",
    "#                 when((col(c) == target_value | col(c).isNull()), lit(not_informed_str)).otherwise(col(c))\n",
    "#             )\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "def non_values_to_not_informed(\n",
    "    df: DataFrame,\n",
    "    columns: list[str] | None,\n",
    "    target_values: list[str],\n",
    "    not_informed_str: str = \"NAO INFORMADO\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converte para 'NAO INFORMADO':\n",
    "      - valores contidos em 'target_values'\n",
    "      - strings com valor \"0\"\n",
    "      - strings vazias (\"\")\n",
    "      - strings contendo apenas espaços\n",
    "      - valores nulos (NULL)\n",
    "    Observações:\n",
    "      - Só aplica em colunas de tipo string.\n",
    "      - Se 'columns' for None, aplica em todas as colunas string do DataFrame.\n",
    "    \"\"\"\n",
    "    # Seleciona colunas string (evita tentar comparar tipos não-string)\n",
    "    string_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "\n",
    "    if columns is None:\n",
    "        target_cols = string_cols\n",
    "    else:\n",
    "        # Filtra apenas colunas string dentre as solicitadas\n",
    "        target_cols = [c for c in columns if c in string_cols]\n",
    "\n",
    "    # Conjunto de valores alvo inclui também \"0\" e \"\" (o trim resolverá espaços)\n",
    "    # Observação: \"\" já é coberto pela condição de trim(col) == \"\"\n",
    "    base_targets = set(target_values or [])\n",
    "    # \"0\" explicitamente pedido pela regra\n",
    "    base_targets.add(\"0\")\n",
    "\n",
    "    for c in target_cols:\n",
    "        # Condições:\n",
    "        # - nulo\n",
    "        # - valor em target_values OU \"0\"\n",
    "        # - vazio após trim (\"\" ou só espaços)\n",
    "        condition = (\n",
    "            col(c).isNull()\n",
    "            | trim(col(c)).isin(list(base_targets))\n",
    "            | (trim(col(c)) == \"\")\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            when(condition, lit(not_informed_str)).otherwise(col(c))\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def eliminate_row_if_null_or_blank(df: DataFrame, columns_to_check: list[str]) -> DataFrame:\n",
    "    \"\"\"Elimina linhas onde as colunas listadas forem nulas ou vazias\"\"\"\n",
    "    for column_name in columns_to_check:\n",
    "        df = df.filter(~(col(column_name).isNull()) & ~(col(column_name) == \"\") & ~(col(column_name) == \" \"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def recommend_salt_simple(df, col):\n",
    "    \"\"\"\n",
    "    Recomenda um valor de salt (número de partições extras) para balancear dados em joins ou writes\n",
    "    \"\"\"\n",
    "    counts = df.groupBy(col).count()\n",
    "\n",
    "    stats = counts.agg(\n",
    "        max(\"count\").alias(\"max_count\"),\n",
    "        percentile_approx(\"count\", 0.5).alias(\"median_count\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    max_c = stats[\"max_count\"]\n",
    "    med_c = stats[\"median_count\"]\n",
    "\n",
    "    if med_c == 0:\n",
    "        return 1\n",
    "\n",
    "    ratio = max_c / med_c\n",
    "\n",
    "    if ratio < 1.5:\n",
    "        salt = 1\n",
    "    elif ratio < 2.5:\n",
    "        salt = 4\n",
    "    elif ratio < 4:\n",
    "        salt = 8\n",
    "    else:\n",
    "        salt = 16\n",
    "\n",
    "    return salt\n",
    "\n",
    "\n",
    "def _estimate_row_size_parquet(df, sample_rows=200_000, tmp_dir=\"dbfs:/tmp/row_size_estimate\"):\n",
    "    \"\"\"Estima o tamanho médio (em bytes) de uma linha de um DataFrame Spark ao ser gravada em formato Parquet\"\"\"\n",
    "    tmp_path = f\"{tmp_dir}/est_{uuid.uuid4().hex}\"\n",
    "    \n",
    "    try:\n",
    "        sample = df.limit(sample_rows)\n",
    "        sample.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"snappy\").save(tmp_path)\n",
    "        \n",
    "        files = dbutils.fs.ls(tmp_path)\n",
    "        total_bytes = builtins.sum(f.size for f in files if f.name.endswith(\".parquet\"))\n",
    "        \n",
    "        row_count = sample.count()\n",
    "        avg_bytes = total_bytes / row_count if row_count > 0 else 0.0\n",
    "        \n",
    "        return avg_bytes\n",
    "    finally:\n",
    "        try:\n",
    "            dbutils.fs.rm(tmp_path, recurse=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def recommend_partitions(df, desired_file_mb=128, sample_rows=200_000):\n",
    "    \"\"\"\n",
    "    Recomenda o número de partições para atingir um tamanho de arquivo desejado\n",
    "    \"\"\"\n",
    "    avg_row_bytes = _estimate_row_size_parquet(df, sample_rows=sample_rows)\n",
    "    total_rows = df.count()\n",
    "    desired_bytes = desired_file_mb * 1024 * 1024\n",
    "    total_bytes = total_rows * avg_row_bytes\n",
    "\n",
    "    num_partitions = builtins.max(1, int(total_bytes / desired_bytes)) if desired_bytes > 0 else 1\n",
    "\n",
    "    return {\n",
    "        \"avg_row_bytes\": float(avg_row_bytes),\n",
    "        \"total_rows\": int(total_rows),\n",
    "        \"total_mb_estimated\": float(total_bytes / (1024*1024)),\n",
    "        \"desired_file_mb\": int(desired_file_mb),\n",
    "        \"recommended_partitions\": int(num_partitions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe5ef22-31e6-4d98-a97b-6c00339c5685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos CNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ec740b-b36a-4221-8280-51c6bf4b7433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES CNAE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_cnae = obter_data_ultima_transformacao(\"cnaes_tratado\")\n",
    "\n",
    "cnaes_df = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/cnaes\")\n",
    "\n",
    "if ultima_transformacao_cnae:\n",
    "    cnaes_df = cnaes_df.filter(col(\"ingestion_dt\") > ultima_transformacao_cnae)\n",
    "    print(f\"Filtrando CNAE desde: {ultima_transformacao_cnae}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os CNAE\")\n",
    "\n",
    "print(f\"Total de registros CNAE para processar: {cnaes_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff4d5b8-887e-4a7d-98f7-70bc2de1428c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sequência de tratamentos\n",
    "cnaes_df = strip_df(cnaes_df, [\"descricao\"])\n",
    "cnaes_df = normalize_blanks_to_null(cnaes_df, [\"descricao\"])\n",
    "cnaes_df = remove_accents(cnaes_df, [\"descricao\"])\n",
    "\n",
    "cnaes_df = (\n",
    "    cnaes_df\n",
    "    .withColumn(\"descricao\", upper(col(\"descricao\")))\n",
    "    .withColumn(\"codigo\", lpad(col(\"codigo\"), 7, \"0\"))\n",
    "    .drop(\"origin_file_name\", \"file_name_only\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "print(f\"CNAE transformados: {cnaes_df.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8767492b-0bf9-4e36-8df0-87a53d38fdf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escrita incremental (append) na Silver\n",
    "(\n",
    "    cnaes_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/cnaes_tratado\")\n",
    ")\n",
    "\n",
    "# Criar tabela se não existir\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.cnaes_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/cnaes_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"cnaes_tratado\", cnaes_df.count())\n",
    "\n",
    "print(\"CNAE salvos em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7783400c-4ff1-4d30-85d8-efa685e927f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fda0604-cef5-4a1f-a9e3-4d3d4655ae44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES EMPRESAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_empresas = obter_data_ultima_transformacao(\"empresas_tratado\")\n",
    "\n",
    "empresas_df = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/empresas\")\n",
    "\n",
    "if ultima_transformacao_empresas:\n",
    "    empresas_df = empresas_df.filter(col(\"ingestion_dt\") > ultima_transformacao_empresas)\n",
    "    print(f\"Filtrando Empresas desde: {ultima_transformacao_empresas}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todas as Empresas\")\n",
    "\n",
    "# display(empresas_df)\n",
    "\n",
    "print(f\"Total de registros Empresas para processar: {empresas_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6737fd7e-8c6f-4039-bbf2-46fbaa03a2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sequência de tratamentos\n",
    "empresas_df = eliminate_row_if_null_or_blank(empresas_df, [\"CNPJ_BASICO\"])\n",
    "empresas_df = empresas_df.filter(~((empresas_df.CNPJ_BASICO == \"00000000\") | (empresas_df.CNPJ_BASICO == \"\") | (empresas_df.CNPJ_BASICO.isNull())))\n",
    "\n",
    "empresas_df = empresas_df.drop(\"anomes\",\"origin_file_name\",\"file_name_only\")\n",
    "\n",
    "colunas_tratamento = [\n",
    "    \"CNPJ_BASICO\", \n",
    "    \"RAZAO_SOCIAL_NOME_EMPRESARIAL\",  \n",
    "    \"NATUREZA_JURIDICA\", \n",
    "    \"QUALIFICACAO_DO_RESPONSAVEL\", \n",
    "    \"CAPITAL_SOCIAL_DA_EMPRESA\", \n",
    "    \"PORTE_DA_EMPRESA\", \n",
    "    \"ENTE_FEDERATIVO_RESPONSAVEL\"\n",
    "]\n",
    "empresas_df = strip_df(empresas_df, colunas_tratamento)\n",
    "empresas_df = remove_accents(empresas_df, colunas_tratamento)\n",
    "empresas_df = normalize_blanks_to_null(empresas_df, colunas_tratamento)\n",
    "\n",
    "empresas_df = empresas_df.withColumn(\"CNPJ_BASICO\", lpad(col('CNPJ_BASICO'), 8, \"0\"))\n",
    "empresas_df = empresas_df.withColumn(\"RAZAO_SOCIAL_NOME_EMPRESARIAL\", upper(col(\"RAZAO_SOCIAL_NOME_EMPRESARIAL\")))\n",
    "empresas_df = empresas_df.withColumn(\"ENTE_FEDERATIVO_RESPONSAVEL\", when(col(\"ENTE_FEDERATIVO_RESPONSAVEL\").isNull(), \"NAO INFORMADO\").otherwise(col(\"ENTE_FEDERATIVO_RESPONSAVEL\")))\n",
    "empresas_df = empresas_df.withColumn(\"PORTE_DA_EMPRESA\", when(col(\"PORTE_DA_EMPRESA\").isNull(), \"NAO INFORMADO\").otherwise(col(\"PORTE_DA_EMPRESA\")))\n",
    "empresas_df = empresas_df.dropDuplicates()\n",
    "\n",
    "print(f\"Empresas transformadas: {empresas_df.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18a1f33-0a3c-464b-a6a4-ee75e6082997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escrita incremental (append) na Silver\n",
    "(\n",
    "    empresas_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/empresas_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.empresas_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/empresas_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"empresas_tratado\", empresas_df.count())\n",
    "\n",
    "print(\"Empresas salvas em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a60e132-3080-40ed-8029-cef68221a82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Estabelecimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ce5930-092e-457e-9e97-988287efbdaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES ESTABELECIMENTOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_estabelecimentos = obter_data_ultima_transformacao(\"estabelecimentos_tratado\")\n",
    "\n",
    "estabelecimentos_df = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/estabelecimentos\")\n",
    "\n",
    "if ultima_transformacao_estabelecimentos:\n",
    "    estabelecimentos_df = estabelecimentos_df.filter(col(\"ingestion_dt\") > ultima_transformacao_estabelecimentos)\n",
    "    print(f\"Filtrando Estabelecimentos desde: {ultima_transformacao_estabelecimentos}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os Estabelecimentos\")\n",
    "\n",
    "print(f\"Total de registros Estabelecimentos para processar: {estabelecimentos_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce5036c-7d5c-4563-ace8-21b57dacb928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sequência de tratamentos\n",
    "estabelecimentos_df = eliminate_row_if_null_or_blank(estabelecimentos_df, [\"CNPJ_BASICO\"])\n",
    "estabelecimentos_df = estabelecimentos_df.filter(~((estabelecimentos_df.CNPJ_BASICO == \"00000000\") | (estabelecimentos_df.CNPJ_BASICO == \"\") | (estabelecimentos_df.CNPJ_BASICO.isNull())))\n",
    "\n",
    "estabelecimentos_df = estabelecimentos_df.drop(\"ingestion_dt\",\"anomes\",\"origin_file_name\",\"file_name_only\")\n",
    "\n",
    "columns_to_remove = [\n",
    "    \"DDD1\", \"TELEFONE1\", \"DDD2\", \"TELEFONE2\", \"DDD_FAX\", \"FAX\", \"CORREIO_ELETRONICO\", \"TIPO_LOGRADOURO\", \"LOGRADOURO\", \"NUMERO\", \"COMPLEMENTO\", \"CEP\",\n",
    "    \"SITUACAO_ESPECIAL\", \"DATA_SITUACAO_ESPECIAL\", 'ingestion_dt'\n",
    "]\n",
    "estabelecimentos_df = estabelecimentos_df.drop(*columns_to_remove)\n",
    "\n",
    "colunas_tratamento = [\"CNPJ_BASICO\", \"CNPJ_ORDEM\", \"CNPJ_DV\", \"NOME_FANTASIA\", \"SITUACAO_CADASTRAL\", \"DATA_SITUACAO_CADASTRAL\", \"MOTIVO_SITUACAO_CADASTRAL\", \"NOME_CIDADE_EXTERIOR\", \"PAIS\", \"DT_INICIO_ATIVIDADE\", \"CNAE_FISCAL_PRINCIPAL\", \"CNAE_FISCAL_SECUNDARIA\", \"BAIRRO\", \"UF\", \"MUNICIPIO\", \"IDENTIFICADOR_MATRIZ_FILIAL\"]\n",
    "estabelecimentos_df = strip_df(estabelecimentos_df, colunas_tratamento)\n",
    "estabelecimentos_df = remove_accents(estabelecimentos_df, colunas_tratamento)\n",
    "estabelecimentos_df = normalize_blanks_to_null(estabelecimentos_df, colunas_tratamento)\n",
    "\n",
    "colunas_tratamento_datas = [\"DATA_SITUACAO_CADASTRAL\", \"DT_INICIO_ATIVIDADE\"]\n",
    "estabelecimentos_df = cast_dates(estabelecimentos_df, colunas_tratamento_datas)\n",
    "\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_BASICO\", lpad(col('CNPJ_BASICO'), 8, \"0\"))\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_ORDEM\", lpad(col('CNPJ_ORDEM'), 4, \"0\"))\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_DV\", lpad(col('CNPJ_DV'), 2, \"0\"))\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_PRINCIPAL\", lpad(col('CNAE_FISCAL_PRINCIPAL'), 7, \"0\"))\n",
    "\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"NOME_FANTASIA\", upper(col(\"NOME_FANTASIA\")))\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"BAIRRO\", upper(col(\"BAIRRO\")))\n",
    "\n",
    "target_values = [\"0\", \"00\"]\n",
    "colunas_tratamento_not_informed = [\"NOME_FANTASIA\", \"NOME_CIDADE_EXTERIOR\", \"PAIS\", \"BAIRRO\"]\n",
    "estabelecimentos_df = non_values_to_not_informed(estabelecimentos_df, colunas_tratamento_not_informed, target_values)\n",
    "\n",
    "# estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_SECUNDARIA_EXP\", explode(split(col(\"CNAE_FISCAL_SECUNDARIA\"), \",\")))\n",
    "# estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_SECUNDARIA_EXP\", lpad(col('CNAE_FISCAL_SECUNDARIA_EXP'), 7, \"0\"))\n",
    "\n",
    "# group_columns = ['CNPJ_BASICO', 'CNPJ_ORDEM', 'CNPJ_DV', 'IDENTIFICADOR_MATRIZ_FILIAL', 'NOME_FANTASIA', 'SITUACAO_CADASTRAL', 'DATA_SITUACAO_CADASTRAL', 'MOTIVO_SITUACAO_CADASTRAL', 'NOME_CIDADE_EXTERIOR', 'PAIS', 'DT_INICIO_ATIVIDADE', 'CNAE_FISCAL_PRINCIPAL', 'BAIRRO', 'UF', 'MUNICIPIO']\n",
    "# estabelecimentos_df = estabelecimentos_df.groupBy(*group_columns).agg(collect_list(\"CNAE_FISCAL_SECUNDARIA_EXP\").alias(\"CNAE_FISCAL_SECUNDARIA_BACK\"))\n",
    "# estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_SECUNDARIA\", concat_ws(\",\", col(\"CNAE_FISCAL_SECUNDARIA_BACK\"))).drop(\"CNAE_FISCAL_SECUNDARIA_BACK\")\n",
    "\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_BASICO_2D\", substring(col(\"CNPJ_BASICO\"), 1, 2))\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"IS_MATRIZ\", when(col(\"IDENTIFICADOR_MATRIZ_FILIAL\") == \"1\", True).otherwise(False))\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"IS_ATIVA\", when(col(\"SITUACAO_CADASTRAL\") == \"02\", True).otherwise(False))\n",
    "\n",
    "# filiais_df = estabelecimentos_df.filter(col(\"IS_MATRIZ\") == False).groupBy(\"CNPJ_BASICO\").count()\n",
    "# filiais_df = filiais_df.withColumnRenamed(\"count\", \"QTDE_FILIAIS\")\n",
    "# estabelecimentos_df = estabelecimentos_df.join(filiais_df, on=\"CNPJ_BASICO\", how=\"left\")\n",
    "\n",
    "recommendations = recommend_partitions(estabelecimentos_df)\n",
    "print(f\"RECOMMENDATIONS: {str(recommendations)}\")\n",
    "partitions_number = builtins.max(1, int(recommendations['recommended_partitions']))\n",
    "\n",
    "salt_buckets = builtins.int(recommend_salt_simple(estabelecimentos_df, \"CNPJ_BASICO_2D\"))\n",
    "salt_buckets = builtins.max(1, salt_buckets)\n",
    "\n",
    "if salt_buckets == 1:\n",
    "    estabelecimentos_df = estabelecimentos_df.withColumn(\"salt\", lit(0).cast(\"int\"))\n",
    "else:\n",
    "    estabelecimentos_df = estabelecimentos_df.withColumn(\n",
    "        \"salt\",\n",
    "        (xxhash64(col(\"CNPJ_BASICO_2D\")) % lit(salt_buckets)).cast(\"int\")\n",
    "    )\n",
    "\n",
    "estabelecimentos_df = estabelecimentos_df.dropDuplicates()\n",
    "\n",
    "estabelecimentos_df = estabelecimentos_df.repartition(partitions_number, col(\"salt\"))\n",
    "\n",
    "print(f\"Estabelecimentos transformados: {estabelecimentos_df.count()} registros\")\n",
    "\n",
    "\n",
    "display(estabelecimentos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f771266-48cb-4387-a82b-619f1eea00b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escrita incremental (append) na Silver\n",
    "(\n",
    "    estabelecimentos_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"salt\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/estabelecimentos_tratado\")\n",
    ")\n",
    "\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS silver.estabelecimentos_tratado\n",
    "#     USING DELTA\n",
    "#     LOCATION '{path_storage_silver}/cnpj/estabelecimentos_tratado'\n",
    "# \"\"\")\n",
    "\n",
    "estabelecimentos_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"silver.estabelecimentos_tratado\")\n",
    "\n",
    "registrar_transformacao_silver(\"estabelecimentos_tratado\", estabelecimentos_df.count())\n",
    "\n",
    "print(\"Estabelecimentos salvos em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fc08962-7a99-4a44-adf9-86fa06a79344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Municípios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17d2317-5a8e-4b1b-8711-64ac28d046a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES MUNICÍPIOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_municipios = obter_data_ultima_transformacao(\"municipios_tratado\")\n",
    "\n",
    "df_municipios = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/municipios\")\n",
    "\n",
    "if ultima_transformacao_municipios:\n",
    "    df_municipios = df_municipios.filter(col(\"ingestion_dt\") > ultima_transformacao_municipios)\n",
    "    print(f\"Filtrando Municípios desde: {ultima_transformacao_municipios}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os Municípios\")\n",
    "\n",
    "print(f\"Total de registros Municípios para processar: {df_municipios.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e3f83d-19b4-4e60-b3d3-62f4938f6978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_tratamento = [\"codigo\", \"descricao\"]\n",
    "df_municipios = strip_df(df_municipios, colunas_tratamento)\n",
    "df_municipios = remove_accents(df_municipios, colunas_tratamento)\n",
    "df_municipios = normalize_blanks_to_null(df_municipios, colunas_tratamento)\n",
    "df_municipios = df_municipios.withColumn(\"descricao\", upper(col(\"descricao\")))\n",
    "df_municipios = df_municipios.dropDuplicates()\n",
    "df_municipios = df_municipios.drop(\"origin_file_name\")\n",
    "df_municipios = df_municipios.drop(\"file_name_only\")\n",
    "\n",
    "print(f\"Municípios transformados: {df_municipios.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aabeb01-8efe-48a8-9723-e026ac649d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_municipios.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/municipios_Tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.municipios_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/municipios_Tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"municipios_tratado\", df_municipios.count())\n",
    "\n",
    "print(\"Municípios salvos em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54276bba-6045-49b3-8ea5-fe28dc935e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Natureza Jurídica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35517f48-7554-4e85-b635-46ea34011620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES NATUREZA JURÍDICA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_natureza = obter_data_ultima_transformacao(\"Naturezas_Tratado\")\n",
    "\n",
    "df_natureza_juridica = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/naturezas\")\n",
    "\n",
    "if ultima_transformacao_natureza:\n",
    "    df_natureza_juridica = df_natureza_juridica.filter(col(\"ingestion_dt\") > ultima_transformacao_natureza)\n",
    "    print(f\"Filtrando Natureza Jurídica desde: {ultima_transformacao_natureza}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todas as Naturezas Jurídicas\")\n",
    "\n",
    "print(f\"Total de registros Natureza Jurídica para processar: {df_natureza_juridica.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1d8cde-a103-4374-81ff-e88d44a754af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_tratamento = [\"codigo\", \"descricao\"]\n",
    "df_natureza_juridica = strip_df(df_natureza_juridica, colunas_tratamento)\n",
    "df_natureza_juridica = remove_accents(df_natureza_juridica, colunas_tratamento)\n",
    "df_natureza_juridica = normalize_blanks_to_null(df_natureza_juridica, colunas_tratamento)\n",
    "df_natureza_juridica = df_natureza_juridica.withColumn(\"descricao\", upper(col(\"descricao\")))\n",
    "df_natureza_juridica = df_natureza_juridica.drop(\"origin_file_name\")\n",
    "df_natureza_juridica = df_natureza_juridica.drop(\"file_name_only\")\n",
    "df_natureza_juridica = df_natureza_juridica.dropDuplicates()\n",
    "\n",
    "print(f\"Natureza Jurídica transformada: {df_natureza_juridica.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45dc5831-fd6e-4914-be06-b17f78466c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_natureza_juridica.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/natureza_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.natureza_juridica_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/natureza_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"natureza_tratado\", df_natureza_juridica.count())\n",
    "\n",
    "print(\"Natureza Jurídica salva em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c695fadb-8c6c-495d-b116-13fa8dfe3d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Países"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414932d8-548a-4416-afc5-2db94b8f3533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES PAÍSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_paises = obter_data_ultima_transformacao(\"Paises_Tratado\")\n",
    "\n",
    "df_paises = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/paises\")\n",
    "\n",
    "if ultima_transformacao_paises:\n",
    "    df_paises = df_paises.filter(col(\"ingestion_dt\") > ultima_transformacao_paises)\n",
    "    print(f\"Filtrando Países desde: {ultima_transformacao_paises}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os Países\")\n",
    "\n",
    "print(f\"Total de registros Países para processar: {df_paises.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a2819c-6686-47b3-84ce-b9ae7b864447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_tratamento = [\"codigo\", \"descricao\"]\n",
    "df_paises = strip_df(df_paises, colunas_tratamento)\n",
    "df_paises = remove_accents(df_paises, colunas_tratamento)\n",
    "df_paises = normalize_blanks_to_null(df_paises, colunas_tratamento)\n",
    "df_paises = df_paises.withColumn(\"descricao\", upper(col(\"descricao\")))\n",
    "df_paises = df_paises.drop(\"origin_file_name\")\n",
    "df_paises = df_paises.drop(\"file_name_only\")\n",
    "df_paises = df_paises.dropDuplicates()\n",
    "\n",
    "print(f\"Países transformados: {df_paises.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b874d07-a0c6-43d1-b50a-7784b8627c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_paises.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/paises_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.paises_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/paises_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"paises_tratado\", df_paises.count())\n",
    "\n",
    "print(\"Países salvos em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b887dbad-65aa-4a89-a6e2-9b74bca3c416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Qualificações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223d9629-45ff-4034-b621-fc07b764ca7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES QUALIFICAÇÕES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_qualificacoes = obter_data_ultima_transformacao(\"Qualificacoes_Tratado\")\n",
    "\n",
    "df_qualificacoes = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/qualificacoes\")\n",
    "\n",
    "if ultima_transformacao_qualificacoes:\n",
    "    df_qualificacoes = df_qualificacoes.filter(col(\"ingestion_dt\") > ultima_transformacao_qualificacoes)\n",
    "    print(f\"Filtrando Qualificações desde: {ultima_transformacao_qualificacoes}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todas as Qualificações\")\n",
    "\n",
    "print(f\"Total de registros Qualificações para processar: {df_qualificacoes.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3163a8bd-bed6-49c9-b55d-2a63a40f1bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_tratamento = [\"codigo\", \"descricao\"]\n",
    "df_qualificacoes = strip_df(df_qualificacoes, colunas_tratamento)\n",
    "df_qualificacoes = remove_accents(df_qualificacoes, colunas_tratamento)\n",
    "df_qualificacoes = normalize_blanks_to_null(df_qualificacoes, colunas_tratamento)\n",
    "df_qualificacoes = df_qualificacoes.withColumn(\"descricao\", upper(col(\"descricao\")))\n",
    "df_qualificacoes = df_qualificacoes.drop(\"origin_file_name\")\n",
    "df_qualificacoes = df_qualificacoes.drop(\"file_name_only\")\n",
    "df_qualificacoes = df_qualificacoes.dropDuplicates()\n",
    "\n",
    "print(f\"Qualificações transformadas: {df_qualificacoes.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e877a14-2f63-45a1-a232-097ae3ead830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_qualificacoes.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/qualificacoes_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.qualificacoes_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/qualificacoes_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"qualificacoes_tratado\", df_qualificacoes.count())\n",
    "\n",
    "print(\"Qualificações salvas em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80cb04ab-a59d-49c6-a861-12865134e91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c57ceed-8cbf-436e-8481-ee0ec58d4e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES SIMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_simples = obter_data_ultima_transformacao(\"simples_tratado\")\n",
    "\n",
    "df_simples = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/simples\")\n",
    "\n",
    "if ultima_transformacao_simples:\n",
    "    df_simples = df_simples.filter(col(\"ingestion_dt\") > ultima_transformacao_simples)\n",
    "    print(f\"Filtrando Simples desde: {ultima_transformacao_simples}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os Simples\")\n",
    "\n",
    "print(f\"Total de registros Simples para processar: {df_simples.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8567591-db41-4b98-80d2-d50e9dd611ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0. Reparticionamento estratégico (logo após leitura da Bronze)\n",
    "df_simples = df_simples.repartition(200, \"CNPJ_BASICO\")\n",
    "\n",
    "# 1. Tratamentos de string (só colunas necessárias)\n",
    "colunas_string = [\"CNPJ_BASICO\", \"OPCAO_PELO_SIMPLES\", \"OPCAO_PELO_MEI\"]\n",
    "df_simples = strip_df(df_simples, colunas_string)\n",
    "df_simples = remove_accents(df_simples, colunas_string)\n",
    "df_simples = normalize_blanks_to_null(df_simples, colunas_string)\n",
    "\n",
    "# 2. Conversão de datas\n",
    "colunas_datas = [\n",
    "    \"DATA_DE_OPCAO_PELO_SIMPLES\",\n",
    "    \"DATA_DE_EXCLUSAO_DO_SIMPLES\",\n",
    "    \"DATA_DE_OPCAO_PELO_MEI\",\n",
    "    \"DATA_DE_EXCLUSAO_DO_MEI\"\n",
    "]\n",
    "df_simples = cast_dates(df_simples, colunas_datas)\n",
    "\n",
    "# 3. Transformações encadeadas + filtro antecipado (reduz volume antes do shuffle)\n",
    "df_simples = (\n",
    "    df_simples\n",
    "    .withColumn(\"CNPJ_BASICO\", lpad(col(\"CNPJ_BASICO\"), 8, \"0\"))\n",
    "    .filter(\n",
    "        col(\"CNPJ_BASICO\").isNotNull() & \n",
    "        (length(col(\"CNPJ_BASICO\")) == 8)\n",
    "    )\n",
    "    .withColumn(\"OPCAO_PELO_SIMPLES\", upper(col(\"OPCAO_PELO_SIMPLES\")))\n",
    "    .withColumn(\"OPCAO_PELO_MEI\", upper(col(\"OPCAO_PELO_MEI\")))\n",
    "    .drop(\"origin_file_name\", \"file_name_only\")\n",
    ")\n",
    "\n",
    "# 4. Deduplicação eficiente com window function \n",
    "w = Window.partitionBy(\"CNPJ_BASICO\").orderBy(\n",
    "    desc(coalesce(\"ingestion_dt\", \"DATA_DE_OPCAO_PELO_SIMPLES\"))\n",
    ")\n",
    "\n",
    "# 4. Count uma vez só\n",
    "total = df_simples.count()\n",
    "print(f\"Simples transformados: {total} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863cdc30-4c16-4803-9f75-a614f1ae63af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_simples.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/simples_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.simples_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/simples_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"simples_tratado\", df_simples.count())\n",
    "\n",
    "print(\"Simples salvos em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5d602b-d469-4854-acf3-5dc6aef142f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamentos Sócios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9354c1b5-ea92-4d9e-ad1a-bce1c5c20757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES SÓCIOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_socios = obter_data_ultima_transformacao(\"socios_tratado\")\n",
    "\n",
    "df_socios = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/socios\")\n",
    "\n",
    "if ultima_transformacao_socios:\n",
    "    df_socios = df_socios.filter(col(\"ingestion_dt\") > ultima_transformacao_socios)\n",
    "    print(f\"Filtrando Sócios desde: {ultima_transformacao_socios}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os Sócios\")\n",
    "\n",
    "print(f\"Total de registros Sócios para processar: {df_socios.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e300016d-8df3-4908-8390-415cf737f0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_tratamento = [\"CNPJ_BASICO\",\"NOME_DO_SOCIO\",\"PAIS\",\"QUALIFICACAO_DO_SOCIO\",\"IDENTIFICADOR_DO_SOCIO\",\"CPF_CNPJ_DO_SOCIO\",\"FAIXA_ETARIA\",\"DATA_DE_ENTRADA_NA_SOCIEDADE\"]\n",
    "df_socios = strip_df(df_socios, colunas_tratamento)\n",
    "df_socios = remove_accents(df_socios, colunas_tratamento)\n",
    "df_socios = normalize_blanks_to_null(df_socios, colunas_tratamento)\n",
    "\n",
    "df_socios = df_socios.withColumn(\"CNPJ_BASICO\", lpad(col('CNPJ_BASICO'), 8, \"0\"))\n",
    "\n",
    "colunas_tratamento_data = [\"DATA_DE_ENTRADA_NA_SOCIEDADE\"]\n",
    "df_socios = cast_dates(df_socios, colunas_tratamento_data)\n",
    "\n",
    "df_socios = df_socios.drop(\"nome_do_representante_legal\",\"qualificacao_do_representante_legal\",\"representante_legal\",\"anomes\",\"ingestion_dt\")\n",
    "\n",
    "df_socios = df_socios.withColumn(\n",
    "    \"PAIS\",\n",
    "    when(\n",
    "        (col(\"PAIS\").isNull()) |\n",
    "        (trim(col(\"PAIS\")) == \"\") |\n",
    "        (col(\"PAIS\") == \"null\"),\n",
    "        \"105\"\n",
    "    ).otherwise(col(\"PAIS\"))\n",
    ")\n",
    "\n",
    "df_socios = df_socios.withColumn(\"NOME_DO_SOCIO\", upper(col(\"NOME_DO_SOCIO\"))) \\\n",
    "                     .withColumn(\"QUALIFICACAO_DO_SOCIO\", upper(col(\"QUALIFICACAO_DO_SOCIO\"))) \\\n",
    "                     .withColumn(\"PAIS\", upper(col(\"PAIS\")))\n",
    "\n",
    "df_socios = df_socios.filter(\n",
    "    col(\"CNPJ_BASICO\").isNotNull() &\n",
    "    (length(col(\"CNPJ_BASICO\")) == 8)\n",
    ")\n",
    "\n",
    "df_socios = df_socios.dropDuplicates([\"CNPJ_BASICO\",\"IDENTIFICADOR_DO_SOCIO\"])\n",
    "df_socios = df_socios.drop('anomes','origin_path_name')\n",
    "\n",
    "print(f\"Sócios transformados: {df_socios.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f7ee21-2b12-42e3-9c53-3cfdf7738f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_socios.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/socios_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.socios_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/socios_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"socios_tratado\", df_socios.count())\n",
    "\n",
    "print(\"Sócios salvos em Silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb884a4-070c-4584-940e-e9f80e08fc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tratamentos Motivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3b6ef9a-7022-4358-822f-aa193904b314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO TRANSFORMAÇÕES MOTIVOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultima_transformacao_motivos = obter_data_ultima_transformacao(\"motivos_tratado\")\n",
    "\n",
    "df_motivos = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/motivos\")\n",
    "\n",
    "if ultima_transformacao_motivos:\n",
    "    df_motivos = df_motivos.filter(col(\"ingestion_dt\") > ultima_transformacao_motivos)\n",
    "    print(f\"Filtrando motivos desde: {ultima_transformacao_motivos}\")\n",
    "else:\n",
    "    print(\"Primeira execução: processando todos os motivos\")\n",
    "\n",
    "print(f\"Total de registros motivos para processar: {df_motivos.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f888b45-db57-44dc-99c8-941d17cb48fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_tratamento = [\"codigo\", \"descricao\"]\n",
    "df_motivos = strip_df(df_motivos, colunas_tratamento)\n",
    "df_motivos = remove_accents(df_motivos, colunas_tratamento)\n",
    "df_motivos = normalize_blanks_to_null(df_motivos, colunas_tratamento)\n",
    "df_motivos = df_motivos.withColumn(\"descricao\", upper(col(\"descricao\")))\n",
    "df_motivos = df_motivos.drop(\"origin_file_name\")\n",
    "df_motivos = df_motivos.drop(\"file_name_only\")\n",
    "df_motivos = df_motivos.dropDuplicates()\n",
    "\n",
    "print(f\"Motivos transformados: {df_motivos.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b915f6-1afb-47eb-b76c-e75fc8c77238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_motivos.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/motivos_tratado\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.motivos_tratado\n",
    "    USING DELTA\n",
    "    LOCATION '{path_storage_silver}/cnpj/motivos_tratado'\n",
    "\"\"\")\n",
    "\n",
    "registrar_transformacao_silver(\"qualificacoes_tratado\", df_motivos.count())\n",
    "\n",
    "print(\"Motivos salvos em Silver\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8739614389901932,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_cnpj",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
