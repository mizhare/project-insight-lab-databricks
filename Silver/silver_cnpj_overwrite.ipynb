{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2bb2bd-a52e-4b9f-8234-f9d2e76a39a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "importacao_libs"
    }
   },
   "outputs": [],
   "source": [
    "import builtins\n",
    "import sys\n",
    "import uuid\n",
    "sys.path.append(\"/Workspace/Users/kgenuins@emeal.nttdata.com/project-insight-lab-databricks\")\n",
    "\n",
    "from Config.spark_config import apply_storage_config\n",
    "from Config.storage_config import *\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,                    \n",
    "    when,                   \n",
    "    lit,                    \n",
    "    trim,                   \n",
    "    upper,                 \n",
    "    lower,                         \n",
    "    coalesce,              \n",
    "    concat,                            \n",
    "    substring,             \n",
    "    length,                \n",
    "    cast,                  \n",
    "    round,                \n",
    "    sum,                   \n",
    "    count,                 \n",
    "    avg,                   \n",
    "    max,                   \n",
    "    min,                   \n",
    "    countDistinct,         \n",
    "    current_timestamp,     \n",
    "    year,                  \n",
    "    month,                 \n",
    "    datediff,              \n",
    "    to_date,               \n",
    "    date_format,           \n",
    "    rank,                  \n",
    "    dense_rank,            \n",
    "    lag,                  \n",
    "    lead,\n",
    "    decode,\n",
    "    encode ,\n",
    "    regexp_replace,\n",
    "    lpad, \n",
    "    translate,\n",
    "    collect_list,\n",
    "    explode,\n",
    "    split, \n",
    "    concat_ws,\n",
    "    percentile_approx,\n",
    "    xxhash64             \n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    DecimalType,\n",
    ")\n",
    "\n",
    "from pyspark.sql import Window, DataFrame\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "apply_storage_config(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75693ee8-348b-4ac9-bbdd-cde95ae1fc41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Localizações dos dados da BRONZE\n",
    "path_storage_bronze = f\"{bronze_path}\"\n",
    "path_storage_silver = f\"{silver_path}\"\n",
    "dbutils.fs.ls(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19cec4ea-523b-4636-9089-f9c50c813782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funções utilitárias do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e68bbce-19c1-4ffb-8919-71a722991cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def latin1_to_utf8(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\" Tenta converter colunas do tipo StringType de iso-8859-1(latin1) para UTF-8 e retorna o dataframe com as colunas convertidas\n",
    "\n",
    "        Parâmetros:\n",
    "        df(DataFrame): Objeto DataFrame com os dados a serem convertidos para UTF-8\n",
    "        columns(list[str]): Lista com os nomes das colunas a serem convertidas\n",
    "\n",
    "        Retorna:\n",
    "        DataFrame: DataFrame com as colunas convertidas para UTF-8\n",
    "    \"\"\"\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(\n",
    "            column_name, \n",
    "            decode(encode(col(column_name), \"ISO-8859-1\"), \"UTF-8\")\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def strip_df(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove espaços em branco do início e do fim das strings nas colunas especificadas de um DataFrame Spark.\n",
    "\n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame de entrada.\n",
    "        columns (list[str]): Lista com os nomes das colunas a serem tratadas.\n",
    "\n",
    "    Retorna:\n",
    "        DataFrame: DataFrame com as colunas especificadas tratadas com trim.\n",
    "    \"\"\"\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(column_name, trim(column_name))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Mapeia e remove acentos.\n",
    "def remove_accents(df: DataFrame, columns: list[str]) -> DataFrame:\n",
    "  \n",
    "    # Mapeamento de acentos\n",
    "    accented = \"áàâãäéèêëíìîïóòôõöúùûüçÁÀÂÃÄÉÈÊËÍÌÎÏÓÒÔÕÖÚÙÛÜÇ\"\n",
    "    unaccented = \"aaaaaeeeeiiiiooooouuuucAAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(\n",
    "            column_name,\n",
    "            regexp_replace(\n",
    "                translate(col(column_name), accented, unaccented),\n",
    "                r\"[^a-zA-Z0-9\\s]\",\n",
    "                \"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#Converte colunas de data para o formato DateType (YYYY-MM-DD).   \n",
    "def cast_dates(df: DataFrame, columns: list[str], input_format: str = \"yyyyMMdd\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converte colunas de string para o tipo DateType em um DataFrame Spark, utilizando o formato de data especificado.\n",
    "\n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame de entrada.\n",
    "        columns (list[str]): Lista com os nomes das colunas a serem convertidas para data.\n",
    "        input_format (str, opcional): Formato da data de entrada (padrão: \"yyyyMMdd\").\n",
    "\n",
    "    Retorna:\n",
    "        DataFrame: DataFrame com as colunas especificadas convertidas para DateType.\n",
    "    \"\"\"\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(\n",
    "            column_name,\n",
    "            to_date(col(column_name).cast(\"string\"), input_format)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_blanks_to_null(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converte para null:\n",
    "      - valores já nulos (None/null)\n",
    "      - strings vazias (\"\")\n",
    "      - strings contendo apenas espaços (\"   \", \"\\t\", etc. após trim)\n",
    "    \n",
    "    Aplica somente em colunas string.\n",
    "    Se 'columns' for None, aplica a todas as colunas string do DataFrame.\n",
    "    Se 'columns' contiver colunas não-string, elas serão ignoradas.\n",
    "    \"\"\"\n",
    "    # Define colunas alvo\n",
    "    if columns is None:\n",
    "        target_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "    else:\n",
    "        # Usa apenas colunas que existem e são string\n",
    "        string_cols = {c for c, t in df.dtypes if t == \"string\"}\n",
    "        target_cols = [c for c in columns if c in string_cols]\n",
    "\n",
    "    # Aplica a normalização\n",
    "    for c in target_cols:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            when(col(c).isNull() | (trim(col(c)) == \"\"), None).otherwise(col(c))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_values_to_not_informed(df: DataFrame, columns: list[str], target_values: list[str], not_informed_str=\"NAO INFORMADO\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converte para 'NAO INFORMADO':\n",
    "      - valores contidos em 'targets'\n",
    "      - strings com valor \"0\"\n",
    "      - strings vazias (\"\")\n",
    "      - strings contendo apenas espaços (\"   \", \"\\t\", etc. após trim)\n",
    "    \n",
    "    Aplica somente em colunas string.\n",
    "    Se 'columns' for None, aplica a todas as colunas string do DataFrame.\n",
    "    Se 'columns' contiver colunas não-string, elas serão ignoradas.\n",
    "    \"\"\"\n",
    "    # Define colunas alvo\n",
    "    if columns is None:\n",
    "        target_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "    else:\n",
    "        # Usa apenas colunas que existem e são string\n",
    "        string_cols = {c for c, t in df.dtypes if t == \"string\"}\n",
    "        target_cols = [c for c in columns if c in string_cols]\n",
    "\n",
    "    print(\"\")\n",
    "    # Aplica a normalização usando a lista de valores 'target_values' como valores exatos a serem substituídos caso encontrados nas colunas\n",
    "    for c in target_cols:\n",
    "        for target_value in target_values:\n",
    "            df = df.withColumn(\n",
    "                c,\n",
    "                when((col(c) == target_value), lit(not_informed_str)).otherwise(col(c))\n",
    "            )\n",
    "    return df\n",
    "    \n",
    "def eliminate_row_if_null_or_blank(df: DataFrame, columns_to_check: list[str]) -> DataFrame:\n",
    "    \"\"\"Elimina linhas onde as colunas listadas no parâmetro 'columns_to_check' for nula ou vazia.\n",
    "    df: objeto DataFrame alvo do tratamento\n",
    "    columns_to_check: lista de colunas a serem verificadas\n",
    "\n",
    "    returns: objeto DataFrame com linhas eliminadas ou o mesmo DataFrame caso não haja linhas a serem eliminadas\n",
    "\n",
    "    \"\"\"\n",
    "    for column_name in columns_to_check:\n",
    "        df = df.filter(~(col(column_name).isNull()) & ~(col(column_name) == \"\") & ~(col(column_name) == \" \"))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def recommend_salt_simple(df, col):\n",
    "    \"\"\"\n",
    "    Recomenda um valor de salt (número de partições extras) para balancear dados em joins ou writes, \n",
    "    baseado na distribuição dos valores de uma coluna.\n",
    "\n",
    "    O algoritmo:\n",
    "      - Conta o número de linhas para cada valor distinto da coluna.\n",
    "      - Calcula o máximo e a mediana dessas contagens.\n",
    "      - Avalia o grau de skew (desequilíbrio) pela razão max/mediana.\n",
    "      - Retorna:\n",
    "          1   se a distribuição for boa (pouco skew)\n",
    "          4   para skew leve\n",
    "          8   para skew moderado\n",
    "          16  para skew forte\n",
    "\n",
    "    \n",
    "    df: DataFrame de entrada.\n",
    "    col: Nome da coluna a ser avaliada.\n",
    "\n",
    "    Retorna:\n",
    "        int: Valor recomendado de salt.\n",
    "    \"\"\"\n",
    "    # conta linhas por valor\n",
    "    counts = df.groupBy(col).count()\n",
    "\n",
    "    # métricas simples\n",
    "    stats = counts.agg(\n",
    "        max(\"count\").alias(\"max_count\"),\n",
    "        percentile_approx(\"count\", 0.5).alias(\"median_count\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    max_c = stats[\"max_count\"]\n",
    "    med_c = stats[\"median_count\"]\n",
    "\n",
    "    if med_c == 0:\n",
    "        return 1  # nada a fazer\n",
    "\n",
    "    ratio = max_c / med_c\n",
    "\n",
    "    # regra simples\n",
    "    if ratio < 1.5:\n",
    "        salt = 1        # distribuição boa\n",
    "    elif ratio < 2.5:\n",
    "        salt = 4        # skew leve\n",
    "    elif ratio < 4:\n",
    "        salt = 8        # skew moderado (seu caso)\n",
    "    else:\n",
    "        salt = 16       # skew forte\n",
    "\n",
    "    return salt\n",
    "\n",
    "\n",
    "def _estimate_row_size_parquet(df, sample_rows=200_000, tmp_dir=\"dbfs:/tmp/row_size_estimate\"):\n",
    "    \"\"\"\n",
    "    Estima o tamanho médio (em bytes) de uma linha de um DataFrame Spark ao ser gravada em formato Parquet.\n",
    "\n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame Spark de entrada.\n",
    "        sample_rows (int, opcional): Número de linhas a serem amostradas para a estimativa. Padrão: 200_000.\n",
    "        tmp_dir (str, opcional): Diretório temporário no DBFS para salvar o arquivo Parquet amostrado. Padrão: \"dbfs:/tmp/row_size_estimate\".\n",
    "\n",
    "    Retorna:\n",
    "        float: Tamanho médio estimado de uma linha em bytes. Retorna 0.0 se não houver linhas.\n",
    "\n",
    "    Observações:\n",
    "        - Utiliza compressão 'snappy' ao gravar o Parquet.\n",
    "        - Remove os arquivos temporários após a estimativa.\n",
    "        - Usa builtins.sum para evitar conflitos com funções Spark.\n",
    "    \"\"\"\n",
    "    tmp_path = f\"{tmp_dir}/est_{uuid.uuid4().hex}\"\n",
    "    (df.limit(int(sample_rows))\n",
    "       .coalesce(1)\n",
    "       .write.mode(\"overwrite\")\n",
    "       .option(\"compression\", \"snappy\")\n",
    "       .parquet(tmp_path))\n",
    "\n",
    "    n = spark.read.parquet(tmp_path).count()\n",
    "\n",
    "    files = dbutils.fs.ls(tmp_path)\n",
    "    # USE o builtins.sum (não o sum do Spark)\n",
    "    total_bytes = builtins.sum([f.size for f in files if f.size is not None])\n",
    "\n",
    "    dbutils.fs.rm(tmp_path, recurse=True)\n",
    "    return (total_bytes / n) if n > 0 else 0.0\n",
    "\n",
    "\n",
    "def recommend_partitions(df, desired_file_mb=256, sample_rows=200_000):\n",
    "    \"\"\"\n",
    "    Recomenda o número de partições para gravar um DataFrame Spark, visando arquivos Parquet próximos ao tamanho desejado.\n",
    "\n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame Spark de entrada.\n",
    "        desired_file_mb (int, opcional): Tamanho desejado de cada arquivo Parquet em megabytes. Padrão: 256.\n",
    "        sample_rows (int, opcional): Número de linhas a serem amostradas para estimar o tamanho médio da linha. Padrão: 200_000.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Dicionário com as seguintes chaves:\n",
    "            - avg_row_bytes (float): Tamanho médio estimado de uma linha em bytes.\n",
    "            - total_rows (int): Número total de linhas do DataFrame.\n",
    "            - total_mb_estimated (float): Tamanho total estimado dos dados em megabytes.\n",
    "            - desired_file_mb (int): Tamanho desejado de cada arquivo em megabytes.\n",
    "            - recommended_partitions (int): Número recomendado de partições para atingir o tamanho de arquivo desejado.\n",
    "    \"\"\"\n",
    "    avg_row_bytes = _estimate_row_size_parquet(df, sample_rows=sample_rows)\n",
    "    total_rows = df.count()\n",
    "    desired_bytes = desired_file_mb * 1024 * 1024\n",
    "    total_bytes = total_rows * avg_row_bytes\n",
    "\n",
    "    # Também use builtins.max para evitar conflitos se tiver importado max do Spark\n",
    "    num_partitions = builtins.max(1, int(total_bytes / desired_bytes)) if desired_bytes > 0 else 1\n",
    "\n",
    "    return {\n",
    "        \"avg_row_bytes\": float(avg_row_bytes),\n",
    "        \"total_rows\": int(total_rows),\n",
    "        \"total_mb_estimated\": float(total_bytes / (1024*1024)),\n",
    "        \"desired_file_mb\": int(desired_file_mb),\n",
    "        \"recommended_partitions\": int(num_partitions),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc59af9-6b49-46f0-b60a-fd23b16374b4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770397800815}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cnae - Tratamento e filtros\n",
    "\n",
    "# Abrindo o arquivo dos Cnaes na Bronze\n",
    "cnaes_df = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Cnaes\")\n",
    "\n",
    "# Recuperando as colunas\n",
    "columns = cnaes_df.columns\n",
    "\n",
    "# -- Sequência de tratamentos --\n",
    "\n",
    "# strip/trim\n",
    "cnaes_df = strip_df(cnaes_df, [\"descricao\"])\n",
    "\n",
    "# Colunas em branco ou somente com espaços terão valor nulificado\n",
    "cnaes_df = normalize_blanks_to_null(cnaes_df, [\"descricao\"])\n",
    "\n",
    "# LPAD dos cnaes\n",
    "cnaes_df = cnaes_df.withColumn(\"codigo\", lpad(col('codigo'), 7, \"0\") )\n",
    "\n",
    "# Dropando duplicados\n",
    "cnaes_df = cnaes_df.dropDuplicates()\n",
    "\n",
    "display(cnaes_df)\n",
    "\n",
    "# Salvando no storage da silver\n",
    "(\n",
    "    cnaes_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/Cnaes_tratado\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbe1bb3-8b6e-4a06-82ca-2295fc4b5885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- Empresas - Tratamento e filtros --\n",
    "\n",
    "# Abrindo o arquivo dos Empresas na Bronze\n",
    "empresas_df = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Empresas\")\n",
    "\n",
    "# Recuperando as colunas\n",
    "columns = empresas_df.columns\n",
    "\n",
    "# -- Sequência de tratamentos --\n",
    "\n",
    "# strip/trim\n",
    "target_columns = [\"RAZAO_SOCIAL_NOME_EMPRESARIAL\"]\n",
    "empresas_df = strip_df(empresas_df, target_columns)\n",
    "\n",
    "# Colunas em branco ou somente com espaços terão valor nulificado\n",
    "empresas_df = normalize_blanks_to_null(empresas_df, target_columns)\n",
    "\n",
    "# LPAD para o cnpj\n",
    "empresas_df = empresas_df.withColumn(\"CNPJ_BASICO\", lpad(empresas_df.CNPJ_BASICO, 8, \"0\"))\n",
    "\n",
    "# Colocando a pontuação correta para o decimal(substituindo ',' por '.') e convertendo para decimal\n",
    "empresas_df = empresas_df.withColumn(\"CAPITAL_SOCIAL_DA_EMPRESA\", regexp_replace(empresas_df.CAPITAL_SOCIAL_DA_EMPRESA, \",\", \".\")) \\\n",
    "                .withColumn(\"CAPITAL_SOCIAL_DA_EMPRESA\", col(\"CAPITAL_SOCIAL_DA_EMPRESA\").cast(DecimalType(18, 2)))\n",
    "\n",
    "# Substituindo valores nulos na coluna 'ENTE_FEDERATIVO_RESPONSAVEL' por \"NAO INFORMADO\"\n",
    "empresas_df = empresas_df.withColumn(\"ENTE_FEDERATIVO_RESPONSAVEL\", when(col(\"ENTE_FEDERATIVO_RESPONSAVEL\").isNull(), \"NAO INFORMADO\").otherwise(col(\"ENTE_FEDERATIVO_RESPONSAVEL\")))\n",
    "\n",
    "# Substituindo valores nulos na coluna 'PORTE_DA_EMPRESA' por \"NAO INFORMADO\"\n",
    "empresas_df = empresas_df.withColumn(\"PORTE_DA_EMPRESA\", when(col(\"PORTE_DA_EMPRESA\").isNull(), \"NAO INFORMADO\").otherwise(col(\"PORTE_DA_EMPRESA\")))\n",
    "\n",
    "# Dropando duplicados\n",
    "empresas_df = empresas_df.dropDuplicates()\n",
    "\n",
    "display(empresas_df)\n",
    "\n",
    "# Salvando os dados no storage da silver\n",
    "(\n",
    "    empresas_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/Empresas_tratado\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b001a4f-268f-4baf-973b-dd106d655264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- Estabelecimentos - Tratamento e filtros --\n",
    "\n",
    "# Abrindo o arquivo dos Estabelecimentos na Bronze\n",
    "estabelecimentos_df = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Estabelecimentos\")\n",
    "\n",
    "# # Recuperando as colunas\n",
    "# columns = estabelecimentos_df.columns\n",
    "\n",
    "\n",
    "# -- Sequência de tratamentos --\n",
    "\n",
    "# Eliminando linhas onde a coluna CNPJ_BASICO é nulo, string vazia ou contém somente espaços em branco\n",
    "# estabelecimentos_df = estabelecimentos_df.filter(~(col(\"CNPJ_BASICO\").isNull()) & ~(col(\"CNPJ_BASICO\") == \"\") & ~(col(\"CNPJ_BASICO\") == \" \"))\n",
    "estabelecimentos_df = eliminate_row_if_null_or_blank(estabelecimentos_df, [\"CNPJ_BASICO\"])\n",
    "\n",
    "# Eliminando linhas com CNPJ_BASICO inválido\n",
    "estabelecimentos_df = estabelecimentos_df.filter(~((estabelecimentos_df.CNPJ_BASICO == \"00000000\") | (estabelecimentos_df.CNPJ_BASICO == \"\") | (estabelecimentos_df.CNPJ_BASICO.isNull()) ))\n",
    "\n",
    "# Removendo colunas conforme acordado como José\n",
    "columns_to_remove = [\n",
    "    \"DDD_1\", \"TELEFONE_1\", \"DDD_2\", \"TELEFONE_2\", \"DDD_FAX\", \"FAX\", \"CORREIO_ELETRONICO\", \"TIPO_LOGRADOURO\", \"LOGRADOURO\", \"NUMERO\", \"COMPLEMENTO\", \"CEP\",\n",
    "    \"SITUACAO_ESPECIAL\", \"DATA_SITUACAO_ESPECIAL\", 'ingestion_dt'\n",
    "]\n",
    "estabelecimentos_df = estabelecimentos_df.drop(*columns_to_remove)\n",
    "\n",
    "# strip/trim\n",
    "target_columns = [\"NOME_FANTASIA\", \"NOME_CIDADE_EXTERIOR\", \"PAIS\", \"BAIRRO\", \"UF\", \"MUNICIPIO\", 'CNAE_FISCAL_PRINCIPAL', 'CNAE_FISCAL_SECUNDARIA']\n",
    "estabelecimentos_df = strip_df(estabelecimentos_df, target_columns)\n",
    "\n",
    "# Colunas em branco ou somente com espaços terão valor nulificado\n",
    "estabelecimentos_df = normalize_blanks_to_null(estabelecimentos_df, target_columns)\n",
    "\n",
    "# LPAD na coluna CNPJ_BASICO\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_BASICO\", lpad(estabelecimentos_df.CNPJ_BASICO, 8, \"0\"))\n",
    "\n",
    "# LPAD na coluna LPDAD CNPJ_ORDEM\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_ORDEM\", lpad(estabelecimentos_df.CNPJ_ORDEM, 4, \"0\"))\n",
    "\n",
    "# LPAD na coluna CNPJ_DV\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_DV\", lpad(estabelecimentos_df.CNPJ_DV, 2, \"0\"))\n",
    "\n",
    "# Colunas com valor nulo/NULL que receberão \"NAO INFORMADO\"\n",
    "null_columns_target = [\"NOME_FANTASIA\", \"SITUACAO_CADASTRAL\", \"MOTIVO_SITUACAO_CADASTRAL\", \"NOME_CIDADE_EXTERIOR\", \"PAIS\", \"BAIRRO\", \"UF\", \"MUNICIPIO\"]\n",
    "estabelecimentos_df = estabelecimentos_df.fillna(\"NAO INFORMADO\", null_columns_target)\n",
    "\n",
    "# Convertendo colunas string que armazenam data usando a função 'cast_dates' \n",
    "target_columns = [\"DATA_SITUACAO_CADASTRAL\", \"DT_INICIO_ATIVIDADE\"]\n",
    "estabelecimentos_df = cast_dates(estabelecimentos_df, target_columns)\n",
    "\n",
    "# Tratando o CNAE_FISCAL_PRINCIPAL usando lpad 7\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_PRINCIPAL\", lpad(col(\"CNAE_FISCAL_PRINCIPAL\"), 7, \"0\"))\n",
    "\n",
    "# -- Tratamento da coluna CNAE_FISCAL_SECUNDARIO --\n",
    "# Aplicando a função 'explode' na coluna CNAE_FISCAL_SECUNDARIO e pegando uma linha por item da lista que está em formato string separado por ','\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_SECUNDARIA_EXP\", explode(split(col(\"CNAE_FISCAL_SECUNDARIA\"), \",\")))\n",
    "\n",
    "# Aplicando a função \"lpad\" para cada um dos CNAE_FISCAL_SECUNDARIO \n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_SECUNDARIA_EXP\", lpad(col(\"CNAE_FISCAL_SECUNDARIA_EXP\"), 7, \"0\")) \n",
    "# print(f\"DEBUG 1 {str(estabelecimentos_df.columns)}\")\n",
    "\n",
    "# Colunas do group_by para agrupar os dados novamente e obter uma lista dos CNAE_FISCAL_SECUNDARIO\n",
    "group_columns =['CNPJ_BASICO', 'CNPJ_ORDEM', 'CNPJ_DV', 'IDENTIFICADOR_MATRIZ_FILIAL', 'NOME_FANTASIA', 'SITUACAO_CADASTRAL', 'DATA_SITUACAO_CADASTRAL', 'MOTIVO_SITUACAO_CADASTRAL', 'NOME_CIDADE_EXTERIOR', 'PAIS', 'DT_INICIO_ATIVIDADE', 'CNAE_FISCAL_PRINCIPAL', 'BAIRRO', 'UF', 'MUNICIPIO', 'origin_path_name', 'anomes']\n",
    "\n",
    "# Agrupando para obter novamente uma lista da coluna CNAE_FISCAL_SECUNDARIA_EXP, que já tem o tratamento feito\n",
    "estabelecimentos_df = estabelecimentos_df.groupBy(*group_columns).agg(collect_list(\"CNAE_FISCAL_SECUNDARIA_EXP\").alias(\"CNAE_FISCAL_SECUNDARIA_BACK\"))\n",
    "\n",
    "# Concatenando CNAE_FISCAL_SECUNDARIA_BACK usando concat_ws para voltar o formato original do dado mas com o conteúdo tratado\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNAE_FISCAL_SECUNDARIA\", concat_ws(\",\", col(\"CNAE_FISCAL_SECUNDARIA_BACK\"))) \\\n",
    "                        .drop(\"CNAE_FISCAL_SECUNDARIA_BACK\")\n",
    "\n",
    "# Partição sintética com os dois primeiros dígitos de CNPJ_BASICO\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"CNPJ_BASICO_2D\", substring(col(\"CNPJ_BASICO\"), 1, 2))\n",
    "\n",
    "# -- Flags úteis para estabelecimento --\n",
    "\n",
    "# Adicionando a 'flag' 'IS_MATRIZ'(True se for matriz. Senão, False)\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"IS_MATRIZ\", when(col(\"IDENTIFICADOR_MATRIZ_FILIAL\") == \"1\", True).otherwise(False))\n",
    "\n",
    "# Adicionando a 'flag' 'IS_ATIVA' (True se o estabelecimento estiver ativo. Senão, False)\n",
    "estabelecimentos_df = estabelecimentos_df.withColumn(\"IS_ATIVA\", when(col(\"SITUACAO_CADASTRAL\") == \"02\", True).otherwise(False))\n",
    "\n",
    "# -- Tratamento das filiais --\n",
    "# Contando as filiais por CNPJ_BASICO usando a coluna 'IS_MATRIZ' com valor False\n",
    "filiais_df = estabelecimentos_df.filter(col(\"IS_MATRIZ\") == False).groupBy(\"CNPJ_BASICO\").count()\n",
    "filiais_df = filiais_df.withColumnRenamed(\"count\", \"QTDE_FILIAIS\")\n",
    "\n",
    "# Adicionando a coluna QTDE_FILIAIS em estabelecimentos_df usando filiais_df e atribuindo valor 0 quando for uma filial\n",
    "estabelecimentos_df = estabelecimentos_df.join(filiais_df, on=\"CNPJ_BASICO\", how=\"left\").fillna(0, subset=['QTDE_FILIAIS'])\n",
    "\n",
    "# display(estabelecimentos_df)\n",
    "\n",
    "# -- Salvando os dados no storage da \"Silver\" --\n",
    "\n",
    "# Calculando o número de partições\n",
    "recommendations = recommend_partitions(estabelecimentos_df)\n",
    "print(f\"RECOMMENDATIONS: {str(recommendations)}\")\n",
    "partitions_number = builtins.max(1, int(recommendations['recommended_partitions']))\n",
    "\n",
    "# Calculando o 'salt' para a distribuição de partições/buckets\n",
    "# salt_buckets = recommend_salt_simple(estabelecimentos_df, \"CNPJ_BASICO_2D\")\n",
    "salt_buckets = builtins.int(recommend_salt_simple(estabelecimentos_df, \"CNPJ_BASICO_2D\"))\n",
    "salt_buckets = builtins.max(1, salt_buckets)\n",
    "\n",
    "# Criando a coluna com o salt para o reparticionamento\n",
    "# estabelecimentos_df = estabelecimentos_df.withColumn(\"salt\", (xxhash64(col(\"CNPJ_BASICO_2D\")) % lit(salt_buckets)).cast(\"int\"))\n",
    "if salt_buckets == 1:\n",
    "    estabelecimentos_df = estabelecimentos_df.withColumn(\"salt\", lit(0).cast(\"int\"))\n",
    "else:\n",
    "    estabelecimentos_df = estabelecimentos_df.withColumn(\n",
    "        \"salt\",\n",
    "        (xxhash64(col(\"CNPJ_BASICO_2D\")) % lit(salt_buckets)).cast(\"int\")\n",
    "    )\n",
    "\n",
    "# Reparticionando o dataframe usando a recomendação de reparticionamento\n",
    "estabelecimentos_df = estabelecimentos_df.repartition(partitions_number, col(\"salt\"))\n",
    "\n",
    "# Gravando os dados no storage da \"Silver\"\n",
    "(\n",
    "    estabelecimentos_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"anomes\", \"salt\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/Estabelecimentos_tratado\")\n",
    ")\n",
    " \n",
    "# Sugestão modo overwriteSchema simples\n",
    "# (\n",
    "#     estabelecimentos_df.write\n",
    "#     .format(\"delta\")\n",
    "#     .mode(\"overwrite\")\n",
    "#     .partitionBy(\"anomes\", \"CNPJ_BASICO_2D\")\n",
    "#     .option(\"overwriteSchema\", \"true\").save(f\"{path_storage_silver}/cnpj/Estabelecimentos_tratado\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ff8860-1f7d-45a7-8a2b-7beebff3811e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770578201172}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Contando as filiais por CNPJ_BASICO quando CNPJ_ORDEM não terminmar com '1' usando a coluna 'IS_MATRIZ' com valor False\n",
    "# filiais_df = estabelecimentos_df.filter(col(\"IS_MATRIZ\") == False).groupBy(\"CNPJ_BASICO\").count()\n",
    "# filiais_df = filiais_df.withColumnRenamed(\"count\", \"QTDE_FILIAIS\")\n",
    "\n",
    "# # Adicionando a coluna QTDE_FILIAIS em estabelecimentos_df usando filiais_df e atribuindo valor 0 quando for uma filial\n",
    "# estabelecimentos_df = estabelecimentos_df.join(filiais_df, on=\"CNPJ_BASICO\", how=\"left\").fillna(0, subset=\"QTDE_FILIAIS\")\n",
    "\n",
    "# display(estabelecimentos_df)\n",
    "\n",
    "# Contando o campo CNPJ_BASICO_2D para verificar a distribuição dos dois primeiros dígitos de CNPJ_BASICO\n",
    "display(estabelecimentos_df.groupBy(\"CNPJ_BASICO_2D\").count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5129fda-5b39-4bd0-8c07-fee757ed6cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Motivos - Tratamento e filtros --\n",
    "df_motivos = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Motivos\")\n",
    "\n",
    "# Recuperando as colunas\n",
    "columns = df_motivos.columns\n",
    "\n",
    "# -- Sequência de tratamentos --\n",
    "\n",
    "# strip/trim\n",
    "df_motivos = strip_df(df_motivos, [\"descricao\"])\n",
    "\n",
    "# Colunas em branco ou somente com espaços terão valor nulificado\n",
    "df_motivos = normalize_blanks_to_null(df_motivos, [\"descricao\"])\n",
    "\n",
    "display(df_motivos)\n",
    "\n",
    "# Salvando os dados no storage da \"Silver\"\n",
    "(\n",
    "    df_motivos.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/Motivos_tratado\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7720e776-e644-45d2-a90e-922aba7768ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Municipios - Tratamento e filtros --\n",
    "df_municipios = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Municipios\")\n",
    "\n",
    "# Recuperando as colunas\n",
    "columns = df_municipios.columns\n",
    "\n",
    "# -- Sequência de tratamentos --\n",
    "# 1. UTF-8\n",
    "# df_municipios = latin1_to_utf8(df_municipios, [\"descricao\"])\n",
    "\n",
    "# 2. strip/trim\n",
    "df_municipios = strip_df(df_municipios, [\"descricao\"])\n",
    "\n",
    "# 3. Colunas em branco ou somente com espaços terão valor nulificado\n",
    "df_municipios = normalize_blanks_to_null(df_municipios, [\"descricao\"])\n",
    " \n",
    "# Tratando o CNAE_FISCAL_PRIMARIO usando lpad\n",
    "df_municipios = df_municipios.withColumn(\"codigo\", lpad(col(\"codigo\"), 4, \"0\"))\n",
    "\n",
    "display(df_municipios)\n",
    "\n",
    "# Salvando os dados no storage da \"Silver\"\n",
    "(\n",
    "    df_motivos.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{path_storage_silver}/cnpj/Municipios_tratado\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cbbad8-1eea-4212-bce3-0294663c63a3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"DESCRICAO\":466},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770649958370}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Naturezas - Tratamentos e Filtro --\n",
    "\n",
    "#1. Lendo os arquivos delta de naturezas\n",
    "df_naturezas = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Naturezas\")\n",
    "\n",
    "colunas_tratamento = [\"DESCRICAO\",\"CODIGO\"]\n",
    "\n",
    "# 2. strip/trim\n",
    "df_naturezas = strip_df(df_naturezas, colunas_tratamento)\n",
    "\n",
    "# 3. Remove acentos e caracteres especiais\n",
    "df_naturezas = remove_accents(df_naturezas, colunas_tratamento)\n",
    "\n",
    "# 4. Normaliza blanks para null\n",
    "df_naturezas = normalize_blanks_to_null(df_naturezas, colunas_tratamento)\n",
    "\n",
    "# 5. LPAD do código da Natureza juridica para 4 digitos\n",
    "df_naturezas = df_naturezas.withColumn(\"CODIGO\", lpad(col('CODIGO'), 4, \"0\") )\n",
    "\n",
    "# 6. Convertendo a descrição para maiúsculo\n",
    "df_naturezas = df_naturezas.withColumn(\n",
    "    \"DESCRICAO\",\n",
    "    upper(col(\"DESCRICAO\"))\n",
    ")\n",
    "\n",
    "# 7. Aplicando filtros\n",
    "df_naturezas = df_naturezas.filter(\n",
    "    col(\"CODIGO\").isNotNull() &\n",
    "    col(\"DESCRICAO\").isNotNull() &\n",
    "    (length(col(\"CODIGO\")) == 4)\n",
    ")\n",
    "\n",
    "#8. dropando duplicados \n",
    "df_naturezas = df_naturezas.dropDuplicates([\"CODIGO\"])\n",
    "\n",
    "# 9. Dropando ingestion_dt e anomes\n",
    "df_naturezas = df_naturezas.drop('ingestion_dt','anomes',\"origin_path_name\")\n",
    "\n",
    "# Resultado final\n",
    "display(df_naturezas)\n",
    "\n",
    "try:   \n",
    "    # Salvando no storage da silver\n",
    "    (\n",
    "    df_naturezas.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .save(f\"{path_storage_silver}/cnpj/Naturezas_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar storage na Silver: {e}\")\n",
    "\n",
    "try:   \n",
    "   df_naturezas.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"silver.Naturezas_Tratado\")\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar a delta table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2219830f-becd-4312-a138-ba3fa45ee1e6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770644019893}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1770490504725}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paises - Tratamentos e Filtro --\n",
    "\n",
    "#1. Lendo os arquivos delta de Paises\n",
    "df_Paises = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Paises\")\n",
    "\n",
    "colunas_tratamento = [\"DESCRICAO\",\"CODIGO\"]\n",
    "\n",
    "# 2. strip/trim\n",
    "df_Paises = strip_df(df_Paises, colunas_tratamento)\n",
    "\n",
    "# 3. Remove acentos e caracteres especiais\n",
    "df_Paises = remove_accents(df_Paises, colunas_tratamento)\n",
    "\n",
    "# 4. Normaliza blanks para null\n",
    "df_Paises = normalize_blanks_to_null(df_Paises, colunas_tratamento)\n",
    "\n",
    "# 5. LPAD da Natureza juridica para 4 digitos\n",
    "df_Paises = df_Paises.withColumn(\"CODIGO\", lpad(col('CODIGO'), 4, '0'))\n",
    "\n",
    "# 6. Convertendo a descrição para maiúsculo\n",
    "df_Paises = df_Paises.withColumn(\n",
    "    \"DESCRICAO\",\n",
    "    upper(col(\"DESCRICAO\"))\n",
    ")\n",
    "\n",
    "# 7. Aplicando filtros\n",
    "df_Paises = df_Paises.filter(\n",
    "    col(\"CODIGO\").isNotNull() &\n",
    "    col(\"DESCRICAO\").isNotNull() &\n",
    "    (length(col(\"CODIGO\")) == 4)\n",
    ")\n",
    "\n",
    "#8. dropando duplicados \n",
    "df_Paises = df_Paises.dropDuplicates([\"CODIGO\"])\n",
    "\n",
    "# 9. Dropando ingestion_dt e anomes\n",
    "df_Paises = df_Paises.drop('ingestion_dt','anomes','origin_path_name')\n",
    "\n",
    "# Resultado final\n",
    "display(df_Paises)\n",
    "\n",
    "try:   \n",
    "    # Salvando no storage da silver\n",
    "    (\n",
    "    df_Paises.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .save(f\"{path_storage_silver}/cnpj/Paises_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar storage na Silver: {e}\")\n",
    "\n",
    "try:\n",
    "    df_Paises.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"silver.Paises_Tratado\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar a delta table: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09fb81bc-235c-4c8b-989d-10f1523ac64f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770644172841}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Qualificacoes - Tratamentos e Filtro --\n",
    "\n",
    "#1. Lendo os arquivos delta de Qualificacoes\n",
    "df_qualificacoes = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Qualificacoes\")\n",
    "\n",
    "colunas_tratamento = [\"DESCRICAO\", \"CODIGO\"]\n",
    "\n",
    "# 2. strip/trim\n",
    "df_qualificacoes = strip_df(df_qualificacoes, colunas_tratamento)\n",
    "\n",
    "# 3. Remove acentos e caracteres especiais\n",
    "df_qualificacoes = remove_accents(df_qualificacoes, colunas_tratamento)\n",
    "\n",
    "# 4. Normaliza blanks para null\n",
    "df_qualificacoes = normalize_blanks_to_null(df_qualificacoes, colunas_tratamento)\n",
    "\n",
    "# 5. LPAD da Natureza juridica para 4 digitos\n",
    "df_qualificacoes = df_qualificacoes.withColumn(\"CODIGO\", lpad(col('CODIGO'), 4, \"0\") )\n",
    "\n",
    "# 6. Convertendo a descrição para maiúsculo\n",
    "df_qualificacoes = df_qualificacoes.withColumn(\n",
    "    \"DESCRICAO\",\n",
    "    upper(col(\"DESCRICAO\"))\n",
    ")\n",
    "\n",
    "# 7. Removendo linhas duplicadas \n",
    "df_qualificacoes = df_qualificacoes.filter(\n",
    "    col(\"CODIGO\").isNotNull() &\n",
    "    col(\"DESCRICAO\").isNotNull() &\n",
    "    (length(col(\"CODIGO\")) == 4)\n",
    ")\n",
    "\n",
    "#8. dropando duplicados \n",
    "df_qualificacoes = df_qualificacoes.dropDuplicates([\"CODIGO\"])\n",
    "\n",
    "# 9. Dropando ingestion_dt e anomes\n",
    "df_qualificacoes = df_qualificacoes.drop('ingestion_dt','anomes','origin_path_name')\n",
    "\n",
    "# Resultado final\n",
    "display(df_qualificacoes)\n",
    "\n",
    "try:\n",
    "    # Salvando no storage da silver\n",
    "    (\n",
    "        df_qualificacoes.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .save(f\"{path_storage_silver}/cnpj/Qualificacoes_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar storage na Silver: {e}\")\n",
    "\n",
    "try:\n",
    "    # Salvando em delta table\n",
    "    (\n",
    "    df_qualificacoes.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"silver.Qualificacoes_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar em delta table: {e}\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c079e2-e639-4000-8ad5-2870727c8709",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770404970856}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simples - Tratamentos e Filtro --\n",
    "\n",
    "# 1. Lendo os arquivos delta de simples\n",
    "df_simples = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Simples\")\n",
    "\n",
    "# 2. colunas para tratamento de string\n",
    "colunas_tratamento = [\"CNPJ_BASICO\",\"OPCAO_PELO_SIMPLES\",\"OPCAO_PELO_MEI\",\"DATA_DE_OPCAO_PELO_SIMPLES\",\"DATA_DE_EXCLUSAO_DO_SIMPLES\",\"DATA_DE_OPCAO_PELO_MEI\",\"DATA_DE_EXCLUSAO_DO_MEI\"]\n",
    "\n",
    "# 3. strip/trim\n",
    "df_simples = strip_df(df_simples, colunas_tratamento)\n",
    "\n",
    "# 4. Remove acentos e caracteres especiais\n",
    "df_simples = remove_accents(df_simples, colunas_tratamento)\n",
    "\n",
    "# 5. Normaliza blanks para null\n",
    "df_simples = normalize_blanks_to_null(df_simples, colunas_tratamento)\n",
    "\n",
    "# 6. colunas para tratamento de datas\n",
    "colunas_tratamento_datas = [\"DATA_DE_OPCAO_PELO_SIMPLES\",\"DATA_DE_EXCLUSAO_DO_SIMPLES\",\"DATA_DE_OPCAO_PELO_MEI\",\"DATA_DE_EXCLUSAO_DO_MEI\"]\n",
    "\n",
    "# 7. função para tratamentos de data\n",
    "df_simples = cast_dates(df_simples, colunas_tratamento_datas)\n",
    "\n",
    "# 8. LPAD do cnpj_basico para 8 digitos\n",
    "df_simples = df_simples.withColumn(\"CNPJ_BASICO\", lpad(col('CNPJ_BASICO'), 8, \"0\") )\n",
    "\n",
    "# 9. Padronizando colunas de opção pelo MEI\n",
    "df_simples = df_simples.withColumn(\n",
    "    \"OPCAO_PELO_SIMPLES\",\n",
    "    upper(col(\"OPCAO_PELO_SIMPLES\"))\n",
    ").withColumn(\n",
    "    \"OPCAO_PELO_MEI\",\n",
    "    upper(col(\"OPCAO_PELO_MEI\"))\n",
    ")\n",
    "\n",
    "# 10. Aplicando filtros \n",
    "df_simples = df_simples.filter(\n",
    "    col(\"CNPJ_BASICO\").isNotNull() &\n",
    "    (length(col(\"CNPJ_BASICO\")) == 8)\n",
    ")\n",
    "df_simples = df_simples.filter(\n",
    "    col(\"OPCAO_PELO_SIMPLES\").isin(\"S\", \"N\") &\n",
    "    col(\"OPCAO_PELO_MEI\").isin(\"S\", \"N\")\n",
    ")\n",
    "\n",
    "# 11. dropando duplicados \n",
    "df_simples = df_simples.dropDuplicates([\"CNPJ_BASICO\"])\n",
    "\n",
    "# 12. Dropando ingestion_dt e anomes\n",
    "df_simples = df_simples.drop('ingestion_dt','anomes','origin_path_name')\n",
    "\n",
    "# Resultado final\n",
    "display(df_simples)\n",
    "\n",
    "try:\n",
    "    # Salvando no storage da silver\n",
    "    (\n",
    "        df_simples.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .save(f\"{path_storage_silver}/cnpj/Simples_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar storage na Silver: {e}\")\n",
    "\n",
    "try:\n",
    "    # Salvando em delta table\n",
    "    (\n",
    "    df_simples.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"silver.Simples_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar em delta table: {e}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704c16d4-c8e3-4c55-9320-25f6b9b326d0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770493309439}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1770493843579}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Socios - Tratamentos e filtros --\n",
    "\n",
    "# 1. Lendo os arquivos delta de socios\n",
    "df_socios = spark.read.format(\"delta\").load(f\"{path_storage_bronze}/cnpj/Socios\")\n",
    "\n",
    "colunas_tratamento = [\"CNPJ_BASICO\",\"NOME_DO_SOCIO\",\"PAIS\",\"QUALIFICACAO_DO_SOCIO\",\"IDENTIFICADOR_DO_SOCIO\",\"CPF/CNPJ_DO_SOCIO\",\"FAIXA_ETARIA\",\"DATA_DE_ENTRADA_NA_SOCIEDADE\"]\n",
    "\n",
    "# 2. strip/trim\n",
    "df_socios = strip_df(df_socios, colunas_tratamento)\n",
    "\n",
    "# 3. Remove acentos e caracteres especiais\n",
    "df_socios = remove_accents(df_socios, colunas_tratamento)\n",
    "\n",
    "# 4. Normaliza blanks para null\n",
    "df_socios = normalize_blanks_to_null(df_socios, colunas_tratamento)\n",
    "\n",
    "# 5. LPAD do cnpj básico\n",
    "df_socios = df_socios.withColumn(\"CNPJ_BASICO\", lpad(col('CNPJ_BASICO'), 8, \"0\") )\n",
    "\n",
    "# 6. Tratamento de datas \n",
    "colunas_tratamento_data = [\"DATA_DE_ENTRADA_NA_SOCIEDADE\"]\n",
    "\n",
    "df_socios = cast_dates(df_socios, colunas_tratamento_data) \n",
    "\n",
    "# 7. Remoção de colunas\n",
    "df_socios = df_socios.drop(\"nome_do_representante_legal\",\"qualificacao_do_representante_legal\",\"representante_legal\",\"anomes\",\"ingestion_dt\")\n",
    "\n",
    "# 8. Transformando colunas nulas em (105 -> brasil)\n",
    "df_socios = df_socios.withColumn(\n",
    "    \"PAIS\",\n",
    "    when(\n",
    "        (col(\"PAIS\").isNull()) |\n",
    "        (trim(col(\"PAIS\")) == \"\") |\n",
    "        (col(\"PAIS\") == \"null\"),\n",
    "        \"105\"\n",
    "    ).otherwise(col(\"PAIS\"))\n",
    ")\n",
    "\n",
    "# 9. Normalizando os campos\n",
    "df_socios = df_socios.withColumn(\"NOME_DO_SOCIO\", upper(col(\"NOME_DO_SOCIO\"))) \\\n",
    "                     .withColumn(\"QUALIFICACAO_DO_SOCIO\", upper(col(\"QUALIFICACAO_DO_SOCIO\"))) \\\n",
    "                     .withColumn(\"PAIS\", upper(col(\"PAIS\")))\n",
    "\n",
    " \n",
    "# 10. Filtrando só os cnpjs que se enquandram nos requisitos \n",
    "df_socios = df_socios.filter(\n",
    "    col(\"CNPJ_BASICO\").isNotNull() &\n",
    "    (length(col(\"CNPJ_BASICO\")) == 8)\n",
    ")\n",
    "\n",
    "# 11. Removendo duplicados\n",
    "df_socios = df_socios.dropDuplicates([\"CNPJ_BASICO\",\"IDENTIFICADOR_DO_SOCIO\"])\n",
    "\n",
    "# 12. Dropando ingestion_dt e anomes\n",
    "df_socios = df_socios.drop('ingestion_dt','anomes','origin_path_name')\n",
    "\n",
    "# Resultado final\n",
    "display(df_socios)\n",
    "\n",
    "try:\n",
    "    # Salvando no storage da silver\n",
    "    (\n",
    "        df_socios.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .save(f\"{path_storage_silver}/cnpj/Socios_Tratado\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar storage na Silver: {e}\")\n",
    "\n",
    "try:\n",
    "    # Salvando em Delta Table\n",
    "    df_socios.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"silver.Socios_Tratado\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Falha ao salvar delta table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b7236d-2636-473c-bd94-2c7ad55532c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b153c5-fa21-4663-a20c-c505cb29b216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f34a919d-12fb-4d1a-9184-cc80af35bc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "137a5c87-fec4-46e0-be2f-9b7b19806e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56095818-8354-4246-9559-89fb704160a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d7bafe-3efb-4e59-b077-4c62cfc7bf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d046150-ee29-4b07-b1c2-bd37eb849b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f6a8b2b-c7cc-4a2a-b377-69d435cea8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816ef937-04bc-4ba2-84fe-dbe01a13be03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_cnpj_overwrite",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
