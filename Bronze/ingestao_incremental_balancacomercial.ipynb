{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446018e2-d391-4f08-99d8-d8cb58b43908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importação de configurações e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5baf887d-a183-4360-aaa1-17a129cc3fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Users/kgenuins@emeal.nttdata.com/project-insight-lab-databricks\")\n",
    "\n",
    "from Config.spark_config import apply_storage_config\n",
    "from Config.storage_config import *\n",
    "from Utils.bronze_csv_loader import *\n",
    "from Utils.utils import *\n",
    "\n",
    "apply_storage_config(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002127b0-e039-42a3-ba44-62c8a35ee8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from Utils.incremental_bronze import *\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name,\n",
    "    current_timestamp,\n",
    "    lit,\n",
    "    col,\n",
    "    count\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b5e172-46fb-4a7f-9600-438244563d9c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\",\"ano\",\"mes\",\"tipo_operacao\"],\"right\":[]},\"columnSizing\":{\"caminho_arquivo\":185,\"nome_arquivo\":124,\"tipo_operacao\":133,\"mes\":102,\"ano\":90,\"data_processamento\":128,\"num_registros\":144},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770768233209}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1770767877347}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.arquivos_processados_balancacomercial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c8aac4-4f15-4150-b67b-8c2f654f2fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da tabela de controle de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d03dc9-657c-46ba-aa55-52ac8e541c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_tabela_controle_arquivos():\n",
    "    \"\"\"\n",
    "hive_metastore.bronze.arquivos_processados_cnpj    Cria tabela para rastrear arquivos CSV já processados\n",
    "    \"\"\"\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bronze.arquivos_processados_balancacomercial (\n",
    "            caminho_arquivo STRING,\n",
    "            nome_arquivo STRING,\n",
    "            tipo_operacao STRING,\n",
    "            ano STRING,\n",
    "            mes STRING,\n",
    "            data_processamento TIMESTAMP,\n",
    "            num_registros BIGINT\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "    print(\"Tabela de controle de arquivos criada!\")\n",
    "\n",
    "criar_tabela_controle_arquivos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa3551b-a373-4b1b-82d8-8c785281c736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64412b23-54d9-4c22-87e5-e153c0a3ef60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def listar_csvs_recursivo(path_base):\n",
    "    \"\"\"\n",
    "    Lista todos os arquivos CSV recursivamente a partir de um caminho base\n",
    "    \"\"\"\n",
    "    arquivos = []\n",
    "\n",
    "    def _ls(path):\n",
    "        for item in dbutils.fs.ls(path):\n",
    "            if item.isDir():\n",
    "                _ls(item.path)\n",
    "            else:\n",
    "                if item.name.lower().endswith(\".csv\"):\n",
    "                    arquivos.append({\n",
    "                        \"path\": item.path,\n",
    "                        \"name\": item.name\n",
    "                    })\n",
    "\n",
    "    _ls(path_base)\n",
    "    return arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a6c6c2-aa62-461b-a1c1-e4350177c49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def obter_arquivos_novos(input_path):\n",
    "    \"\"\"\n",
    "    Filtra apenas arquivos que ainda não foram processados\n",
    "    Usa o nome do arquivo como chave de controle\n",
    "    \"\"\"\n",
    "    arquivos = listar_csvs_recursivo(input_path)\n",
    "\n",
    "    try:\n",
    "        processados = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT nome_arquivo\n",
    "            FROM bronze.arquivos_processados_balancacomercial\n",
    "        \"\"\").collect()\n",
    "        processados_set = {r.nome_arquivo for r in processados}\n",
    "    except:\n",
    "        processados_set = set()\n",
    "\n",
    "    novos = [a for a in arquivos if a[\"name\"] not in processados_set]\n",
    "\n",
    "    print(f\"Total encontrados: {len(arquivos)}\")\n",
    "    print(f\"Novos para processar: {len(novos)}\")\n",
    "\n",
    "    return novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9670b652-9677-456a-a02e-da4bc399911f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def registrar_arquivo_processado(arquivo, df, tipo_operacao):\n",
    "    \"\"\"\n",
    "    Registra arquivo processado na tabela de controle\n",
    "    Executa APENAS após ingestão bem-sucedida\n",
    "    \"\"\"\n",
    "    num_registros = df.count()\n",
    "\n",
    "    ano = df.select(\"ano\").first()[0] if \"ano\" in df.columns else \"N/A\"\n",
    "    mes = df.select(\"mes\").first()[0] if \"mes\" in df.columns else \"N/A\"\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO bronze.arquivos_processados_balancacomercial\n",
    "        VALUES (\n",
    "            '{arquivo[\"path\"]}',\n",
    "            '{arquivo[\"name\"]}',\n",
    "            '{tipo_operacao}',\n",
    "            '{ano}',\n",
    "            '{mes}',\n",
    "            current_timestamp(),\n",
    "            {num_registros}\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd0f2163-5c34-41d8-b0cd-8786d5d529a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Atribuição de variáveis e paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e5def6-bd52-4f82-aa34-e009aba4c8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TIPO_OPERACAO_EXP = \"EXP\"\n",
    "TIPO_OPERACAO_IMP = \"IMP\"\n",
    "NCM = \"NCM\"\n",
    "TB_AUX = \"TB_AUX\"\n",
    "\n",
    "input_path = f\"{balanca_comercial_path}\"\n",
    "\n",
    "output_path_exp = f\"{bronze_path}balancacomercial/{TIPO_OPERACAO_EXP.lower()}\"\n",
    "output_path_imp = f\"{bronze_path}balancacomercial/{TIPO_OPERACAO_IMP.lower()}\"\n",
    "output_path_ncm = f\"{bronze_path}balancacomercial/ncm\"\n",
    "output_path_tb_aux = f\"{bronze_path}balancacomercial/tb_aux\"\n",
    "\n",
    "print(f\"Input path: {input_path}\")\n",
    "print(f\"Output EXP: {output_path_exp}\")\n",
    "print(f\"Output IMP: {output_path_imp}\")\n",
    "print(f\"Output NCM: {output_path_ncm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73d362c-381c-4ee3-b449-ba7c11cf2089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão EXP (Exportação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07fe27d-ad8c-42f6-a5a4-1b7627e969de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_cols_exp = [\n",
    "    \"CO_ANO\",\"CO_MES\",\"CO_NCM\",\"CO_UNID\",\"CO_PAIS\",\n",
    "    \"SG_UF_NCM\",\"CO_VIA\",\"CO_URF\",\"QT_ESTAT\",\"KG_LIQUIDO\",\"VL_FOB\"\n",
    "]\n",
    "\n",
    "schema_exp = StructType([StructField(c, StringType(), True) for c in expected_cols_exp])\n",
    "\n",
    "# Leitura com read_csv_with_quotes + path_glob_filter\n",
    "df_raw_exp, df_exp_corrupt, exp_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path=input_path,\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=True,\n",
    "    path_glob_filter=\"EXP_[0-9][0-9][0-9][0-9].csv\",\n",
    "    header=True,\n",
    "    schema=schema_exp,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/exp\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"EXP lidos: {df_raw_exp.count()} registros\")\n",
    "print(f\"EXP corrompidos: {df_exp_corrupt.count() if df_exp_corrupt is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f5be14-03c5-4b38-87b6-a9f7e27be877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, input_file_name# Aplicar incremental APÓS leitura\n",
    "\n",
    "df_processados_exp = spark.table(\"bronze.arquivos_processados_balancacomercial\") \\\n",
    "    .filter(col(\"tipo_operacao\") == TIPO_OPERACAO_EXP) \\\n",
    "    .select(\"nome_arquivo\") \\\n",
    "    .distinct()\n",
    "\n",
    "df_novos_exp = (\n",
    "    df_raw_exp\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "# Filtro incremental por nome\n",
    "df_novos_exp = df_novos_exp.join(\n",
    "    df_processados_exp,\n",
    "    df_novos_exp.file_name_only == df_processados_exp.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"EXP novos para processar: {df_novos_exp.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b49d06f8-b1d5-4984-94f1-dcb97f620ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_exp = (\n",
    "    df_novos_exp\n",
    "    .withColumn(\"ano\", col(\"CO_ANO\").cast(\"int\"))\n",
    "    .withColumn(\"mes\", col(\"CO_MES\"))\n",
    "    .withColumn(\"tipo_operacao\", lit(TIPO_OPERACAO_EXP))\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbaa1ce6-ae7f-4dca-895b-befbaf886830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enriquecimento Bronze\n",
    "df_bronze_exp = (\n",
    "    df_novos_exp\n",
    "    .withColumn(\"ano\", col(\"CO_ANO\").cast(\"int\"))\n",
    "    .withColumn(\"mes\", col(\"CO_MES\"))\n",
    "    .withColumn(\"tipo_operacao\", lit(TIPO_OPERACAO_EXP))\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Escrita incremental (append)\n",
    "(\n",
    "    df_bronze_exp\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ano\", \"mes\")\n",
    "    .save(output_path_exp)\n",
    ")\n",
    "\n",
    "print(f\"EXP escritos em Bronze: {df_bronze_exp.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc60b32-bafc-49bd-a4d9-860855ecbd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar arquivos processados (apenas os novos)\n",
    "df_registro_exp = (\n",
    "    df_bronze_exp\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\", \"ano\", \"mes\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    ")\n",
    "\n",
    "for row in df_registro_exp.collect():\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO bronze.arquivos_processados_balancacomercial\n",
    "        VALUES (\n",
    "            '{row.origin_file_name}',\n",
    "            '{row.file_name_only}',\n",
    "            '{TIPO_OPERACAO_EXP}',\n",
    "            '{row.ano}',\n",
    "            '{row.mes}',\n",
    "            current_timestamp(),\n",
    "            {row.num_registros}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "print(\" EXP registrados em controle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ccdc7e-ad05-4850-ac59-b47c7ff56f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão IMP (Importação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced1e668-4d29-4693-98cd-c7d5728a8126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_cols_imp = [\n",
    "    \"CO_ANO\",\"CO_MES\",\"CO_NCM\",\"CO_UNID\",\"CO_PAIS\",\n",
    "    \"SG_UF_NCM\",\"CO_VIA\",\"CO_URF\",\"QT_ESTAT\",\"KG_LIQUIDO\",\"VL_FOB\",\"VL_FRETE\",\"VL_SEGURO\"\n",
    "]\n",
    "\n",
    "schema_imp = StructType([StructField(c, StringType(), True) for c in expected_cols_imp])\n",
    "\n",
    "# Leitura com read_csv_with_quotes + path_glob_filter\n",
    "df_raw_imp, df_imp_corrupt, imp_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path=input_path,\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=True,\n",
    "    path_glob_filter=\"IMP_[0-9][0-9][0-9][0-9].csv\",\n",
    "    header=True,\n",
    "    schema=schema_imp,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/imp\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"IMP lidos: {df_raw_imp.count()} registros\")\n",
    "print(f\"IMP corrompidos: {df_imp_corrupt.count() if df_imp_corrupt is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "324c6a3e-83e9-48f9-a8c5-27c082c85063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar incremental APÓS leitura\n",
    "df_processados_imp = spark.table(\"bronze.arquivos_processados_balancacomercial\") \\\n",
    "    .filter(col(\"tipo_operacao\") == TIPO_OPERACAO_IMP) \\\n",
    "    .select(\"nome_arquivo\") \\\n",
    "    .distinct()\n",
    "\n",
    "df_novos_imp = (\n",
    "    df_raw_imp\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "# Filtro incremental por nome\n",
    "df_novos_imp = df_novos_imp.join(\n",
    "    df_processados_imp,\n",
    "    df_novos_imp.file_name_only == df_processados_imp.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"IMP novos para processar: {df_novos_imp.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c13691-6871-4cc9-8d51-024ae07e5610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enriquecimento Bronze\n",
    "df_bronze_imp = (\n",
    "    df_novos_imp\n",
    "    .withColumn(\"ano\", col(\"CO_ANO\").cast(\"int\"))\n",
    "    .withColumn(\"mes\", col(\"CO_MES\"))\n",
    "    .withColumn(\"tipo_operacao\", lit(TIPO_OPERACAO_IMP))\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Escrita incremental (append)\n",
    "(\n",
    "    df_bronze_imp\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ano\", \"mes\")\n",
    "    .save(output_path_imp)\n",
    ")\n",
    "\n",
    "print(f\"IMP escritos em Bronze: {df_bronze_imp.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4714d5-939b-4903-a563-bbad8dd16264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar arquivos processados (apenas os novos)\n",
    "df_registro_imp = (\n",
    "    df_bronze_imp\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\", \"ano\", \"mes\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    ")\n",
    "\n",
    "for row in df_registro_imp.collect():\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO bronze.arquivos_processados_balancacomercial\n",
    "        VALUES (\n",
    "            '{row.origin_file_name}',\n",
    "            '{row.file_name_only}',\n",
    "            '{TIPO_OPERACAO_IMP}',\n",
    "            '{row.ano}',\n",
    "            '{row.mes}',\n",
    "            current_timestamp(),\n",
    "            {row.num_registros}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "print(\"IMP registrados em controle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482f41f2-9160-40dc-a85d-d0ff2fe0a80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38805514-75de-42e5-bb96-73f015aad7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_cols_ncm = [\n",
    "    \"CO_NCM\",\"CO_UNID\",\"CO_SH6\",\"CO_PPE\",\"CO_PPI\",\"CO_FAT_AGREG\",\n",
    "    \"CO_CUCI_ITEM\",\"CO_CGCE_N3\",\"CO_SIIT\",\"CO_ISIC_CLASSE\",\"CO_EXP_SUBSET\",\n",
    "    \"NO_NCM_POR\",\"NO_NCM_ESP\",\"NO_NCM_ING\"\n",
    "]\n",
    "\n",
    "schema_ncm = StructType([StructField(c, StringType(), True) for c in expected_cols_ncm])\n",
    "\n",
    "# Leitura com read_csv_with_quotes + path_glob_filter\n",
    "df_raw_ncm, df_ncm_corrupt, ncm_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path=input_path,\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=True,\n",
    "    path_glob_filter=\"NCM.csv\",\n",
    "    header=True,\n",
    "    schema=schema_ncm,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/ncm\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"NCM lidos: {df_raw_ncm.count()} registros\")\n",
    "print(f\"NCM corrompidos: {df_ncm_corrupt.count() if df_ncm_corrupt is not None else 0} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d935b39-8ff6-42ff-95bd-e973ec63d3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar incremental APÓS leitura\n",
    "df_processados_ncm = spark.table(\"bronze.arquivos_processados_balancacomercial\") \\\n",
    "    .filter(col(\"tipo_operacao\") == NCM) \\\n",
    "    .select(\"nome_arquivo\") \\\n",
    "    .distinct()\n",
    "\n",
    "df_novos_ncm = (\n",
    "    df_raw_ncm\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "# Filtro incremental por nome\n",
    "df_novos_ncm = df_novos_ncm.join(\n",
    "    df_processados_ncm,\n",
    "    df_novos_ncm.file_name_only == df_processados_ncm.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"NCM novos para processar: {df_novos_ncm.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab44396-d2af-40e5-a918-e0e35d011fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enriquecimento Bronze\n",
    "df_bronze_ncm = (\n",
    "    df_novos_ncm\n",
    "    .withColumn(\"tipo_operacao\", lit(NCM))\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Escrita incremental (append)\n",
    "(\n",
    "    df_bronze_ncm\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ingestion_dt\")\n",
    "    .save(output_path_ncm)\n",
    ")\n",
    "\n",
    "print(f\"NCM escritos em Bronze: {df_bronze_ncm.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762d4060-ad60-4af1-91dd-755ef088dee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar arquivos processados (apenas os novos)\n",
    "df_registro_ncm = (\n",
    "    df_bronze_ncm\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    ")\n",
    "\n",
    "for row in df_registro_ncm.collect():\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO bronze.arquivos_processados_balancacomercial\n",
    "        VALUES (\n",
    "            '{row.origin_file_name}',\n",
    "            '{row.file_name_only}',\n",
    "            '{NCM}',\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            current_timestamp(),\n",
    "            {row.num_registros}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "print(\"NCM registrados em controle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e058762-b181-4c6a-a8b8-ca1384feaa26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão Tabelas Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abcd475-b6e6-4dd8-b537-16def4084129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listar arquivos que não são EXP, IMP ou NCM\n",
    "arquivos_csv = []\n",
    "\n",
    "for f in dbutils.fs.ls(input_path):\n",
    "    nome = f.name\n",
    "\n",
    "    if not nome.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    if re.search(r\"IMP_[0-9]{4}\\.csv$\", nome):\n",
    "        continue\n",
    "\n",
    "    if re.search(r\"EXP_[0-9]{4}\\.csv$\", nome):\n",
    "        continue\n",
    "\n",
    "    if re.search(r\"(?i)ncm\\.csv$\", nome):\n",
    "        continue\n",
    "\n",
    "    arquivos_csv.append(f.path)\n",
    "\n",
    "print(f\"Arquivos auxiliares encontrados: {len(arquivos_csv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944708aa-f8d0-4902-be04-553ee46d8e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar apenas arquivos novos\n",
    "df_processados_aux = spark.table(\"bronze.arquivos_processados_balancacomercial\") \\\n",
    "    .filter(col(\"tipo_operacao\") == TB_AUX) \\\n",
    "    .select(\"nome_arquivo\") \\\n",
    "    .distinct()\n",
    "\n",
    "processados_aux_set = {row.nome_arquivo for row in df_processados_aux.collect()}\n",
    "\n",
    "arquivos_novos_aux = [\n",
    "    f for f in arquivos_csv\n",
    "    if os.path.basename(f) not in processados_aux_set\n",
    "]\n",
    "\n",
    "print(f\"Arquivos auxiliares novos: {len(arquivos_novos_aux)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471e30b6-2202-48a3-a70a-b2e32c79fffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_cols_map = {}\n",
    "\n",
    "for file_path in arquivos_novos_aux:\n",
    "    try:\n",
    "        nome_arquivo = os.path.basename(file_path)\n",
    "        nome_delta = nome_arquivo.replace(\".csv\", \"\").lower()\n",
    "\n",
    "        print(f\"\\n Processando: {nome_arquivo}\")\n",
    "\n",
    "        expected_cols = expected_cols_map.get(nome_delta)\n",
    "\n",
    "        # Leitura robusta com aspas\n",
    "        df_ok, df_corrupt, header_cols = read_csv_with_quotes(\n",
    "            spark=spark,\n",
    "            input_path=file_path,\n",
    "            delimiter=\";\",\n",
    "            encoding=\"iso-8859-1\",\n",
    "            recursive=False,\n",
    "            path_glob_filter=None,\n",
    "            header=True,\n",
    "            schema=None,\n",
    "            expected_cols=expected_cols,\n",
    "            multiline=True,\n",
    "            quote=\"\\\"\",\n",
    "            escape=\"\\\"\",\n",
    "            mode=\"PERMISSIVE\",\n",
    "            corrupt_col=\"_corrupt_record\",\n",
    "            ignore_leading_trailing_ws=True,\n",
    "            quarantine_path=f\"{output_path_tb_aux}/_quarentena/{nome_delta}\",\n",
    "            quarantine_mode=\"overwrite\"\n",
    "        )\n",
    "\n",
    "        # Enriquecimento\n",
    "        df_ok = (\n",
    "            df_ok\n",
    "            .withColumn(\"origin_path_name\", input_file_name())\n",
    "            .withColumn(\"tipo_operacao\", lit(TB_AUX))\n",
    "            .withColumn(\"ingestion_dt\", current_timestamp())\n",
    "        )\n",
    "\n",
    "        # Escrita\n",
    "        output_path_tb = f\"{output_path_tb_aux}/{nome_delta}\"\n",
    "\n",
    "        (\n",
    "            df_ok\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .save(output_path_tb)\n",
    "        )\n",
    "\n",
    "        # Métricas\n",
    "        corrupt_count = df_corrupt.count() if df_corrupt is not None else 0\n",
    "        ok_count = df_ok.count()\n",
    "\n",
    "        print(f\"Delta criado: {nome_delta}\")\n",
    "        print(f\"Registros OK: {ok_count} | Corrompidos: {corrupt_count}\")\n",
    "\n",
    "        # Registrar controle\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO bronze.arquivos_processados_balancacomercial\n",
    "            VALUES (\n",
    "                '{file_path}',\n",
    "                '{nome_arquivo}',\n",
    "                '{TB_AUX}',\n",
    "                'N/A',\n",
    "                'N/A',\n",
    "                current_timestamp(),\n",
    "                {ok_count}\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao processar {file_path}: {e}\")\n",
    "\n",
    "print(\"\\nIngestão de tabelas auxiliares concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a167d6a-4bcb-4210-b55e-d3d1dec58730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resumo da ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d010656-ca5a-41aa-b187-d9dc69b2f27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESUMO DA INGESTÃO INCREMENTAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_resumo = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        tipo_operacao,\n",
    "        COUNT(*) as total_arquivos,\n",
    "        SUM(num_registros) as total_registros,\n",
    "        MAX(data_processamento) as ultima_ingestao\n",
    "    FROM bronze.arquivos_processados_balancacomercial\n",
    "    GROUP BY tipo_operacao\n",
    "    ORDER BY tipo_operacao\n",
    "\"\"\")\n",
    "\n",
    "df_resumo.display()\n",
    "\n",
    "print(\"\\n Ingestão finalizada com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7595135648332562,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_incremental_balancacomercial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
