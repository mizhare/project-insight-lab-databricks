{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3577c8-f0c9-4a4f-8bfa-041a656dbe77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importação de configurações e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f04486-e62e-420f-b7e0-0b9f79a82769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Users/kgenuins@emeal.nttdata.com/project-insight-lab-databricks\")\n",
    "\n",
    "from Config.spark_config import apply_storage_config\n",
    "from Config.storage_config import *\n",
    "from Utils.bronze_csv_loader import *\n",
    "\n",
    "apply_storage_config(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3158c6bd-c848-4ecf-b9fc-81f58871585f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from Utils.incremental_bronze import *\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name,\n",
    "    current_timestamp,\n",
    "    lit,\n",
    "    col,\n",
    "    count,\n",
    "    year,\n",
    "    month,\n",
    "    concat,\n",
    "    regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re, os, zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77b6e3db-fc02-4d89-8ce8-4975892206fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da tabela de controle de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc24763-9d5b-40ac-b4de-3c604ceccf70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_tabela_controle_cnpj():\n",
    "    \"\"\"\n",
    "    Cria tabela para rastrear arquivos CNPJ já processados\n",
    "    \"\"\"\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bronze.arquivos_processados_cnpj (\n",
    "            caminho_arquivo STRING,\n",
    "            nome_arquivo STRING,\n",
    "            tipo_operacao STRING,\n",
    "            data_processamento TIMESTAMP,\n",
    "            num_registros BIGINT\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "    print(\"✔ Tabela de controle de arquivos CNPJ criada!\")\n",
    "\n",
    "criar_tabela_controle_cnpj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dda9a9d-6fdf-4bda-8459-e831e9e3cdd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Descompactação de arquivos ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04d7f19-2d04-4b67-88c7-628fc3ff87ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "input_path = f\"{cnpj_path}\"\n",
    "output_path = \"/dbfs/tmp/deszipados\"\n",
    "\n",
    "# Cria a pasta de destino se não existir\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "def decompress_file(arquivo):\n",
    "    \"\"\"\n",
    "    Descompacta um arquivo zip do DBFS e consolida arquivos por tipo\n",
    "    Empresas1, Empresas2, Empresas3 → Empresas/\n",
    "    \"\"\"\n",
    "    if arquivo.name.endswith(\".zip\"):\n",
    "        try:\n",
    "            # Caminho do zip no DBFS\n",
    "            caminho_zip_dbfs = arquivo.path\n",
    "\n",
    "            # Caminho local temporário para descompactar\n",
    "            caminho_zip_local = f\"/tmp/{arquivo.name}\"\n",
    "\n",
    "            # Copia do DBFS para local\n",
    "            dbutils.fs.cp(caminho_zip_dbfs, f\"file:{caminho_zip_local}\")\n",
    "\n",
    "            # Cria uma subpasta temporária para descompactar\n",
    "            nome_subpasta = os.path.splitext(arquivo.name)[0]\n",
    "            caminho_temp = os.path.join(\"/tmp\", f\"temp_{nome_subpasta}\")\n",
    "            os.makedirs(caminho_temp, exist_ok=True)\n",
    "\n",
    "            # Descompacta na pasta temporária\n",
    "            with zipfile.ZipFile(caminho_zip_local, 'r') as zip_ref:\n",
    "                zip_ref.extractall(caminho_temp)\n",
    "            \n",
    "            # Identificar o tipo base (Empresas, Estabelecimentos, Socios, etc.)\n",
    "            # Remove números do final: Empresas1 → Empresas\n",
    "            tipo_base = ''.join([c for c in nome_subpasta if not c.isdigit()])\n",
    "            \n",
    "            # Criar pasta consolidada\n",
    "            caminho_consolidado = os.path.join(output_path, tipo_base)\n",
    "            os.makedirs(caminho_consolidado, exist_ok=True)\n",
    "            \n",
    "            # Copiar todos os arquivos para a pasta consolidada\n",
    "            arquivos_copiados = 0\n",
    "            for root, dirs, files in os.walk(caminho_temp):\n",
    "                for file in files:\n",
    "                    origem = os.path.join(root, file)\n",
    "                    destino = os.path.join(caminho_consolidado, file)\n",
    "                    \n",
    "                    # Se arquivo já existe, renomear para evitar sobrescrita\n",
    "                    if os.path.exists(destino):\n",
    "                        base, ext = os.path.splitext(file)\n",
    "                        contador = 1\n",
    "                        while os.path.exists(destino):\n",
    "                            destino = os.path.join(caminho_consolidado, f\"{base}_{contador}{ext}\")\n",
    "                            contador += 1\n",
    "                    \n",
    "                    # Usar shutil.copy ao invés de os.rename (cross-device)\n",
    "                    shutil.copy2(origem, destino)\n",
    "                    arquivos_copiados += 1\n",
    "            \n",
    "            print(f\"Descompactado: {arquivo.name} → {tipo_base}/ ({arquivos_copiados} arquivo(s))\")\n",
    "            \n",
    "            # Limpeza\n",
    "            os.remove(caminho_zip_local)\n",
    "            \n",
    "            # Remover pasta temporária\n",
    "            shutil.rmtree(caminho_temp)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao descompactar {arquivo.name}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Lista arquivos na pasta do DBFS\n",
    "arquivos = dbutils.fs.ls(input_path)\n",
    "\n",
    "# Descomprime cada arquivo\n",
    "total_descompactados = sum([1 for arquivo in arquivos if decompress_file(arquivo)])\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"✔ Total de ZIPs descompactados: {total_descompactados}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c469712f-0f86-4a20-b905-aa621a937854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Atribuição de variáveis e paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822afe5b-7a4c-4ac2-a00c-51503aef924c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TIPO_EMPRESAS = \"EMPRESAS\"\n",
    "TIPO_ESTABELECIMENTOS = \"ESTABELECIMENTOS\"\n",
    "TIPO_SIMPLES = \"SIMPLES\"\n",
    "TIPO_SOCIOS = \"SOCIOS\"\n",
    "TIPO_PAISES = \"PAISES\"\n",
    "TIPO_MUNICIPIOS = \"MUNICIPIOS\"\n",
    "TIPO_QUALIFICACOES = \"QUALIFICACOES\"\n",
    "TIPO_NATUREZAS = \"NATUREZAS\"\n",
    "TIPO_CNAES = \"CNAES\"\n",
    "TIPO_MOTIVOS = \"MOTIVOS\"\n",
    "\n",
    "# CORRIGIDO: Usar caminho DBFS\n",
    "input_path_descompactado = \"dbfs:/tmp/deszipados\"  # ← MUDANÇA AQUI\n",
    "\n",
    "output_path_empresas = f\"{bronze_path}cnpj/empresas\"\n",
    "output_path_estabelecimentos = f\"{bronze_path}cnpj/estabelecimentos\"\n",
    "output_path_simples = f\"{bronze_path}cnpj/simples\"\n",
    "output_path_socios = f\"{bronze_path}cnpj/socios\"\n",
    "output_path_paises = f\"{bronze_path}cnpj/paises\"\n",
    "output_path_municipios = f\"{bronze_path}cnpj/municipios\"\n",
    "output_path_qualificacoes = f\"{bronze_path}cnpj/qualificacoes\"\n",
    "output_path_naturezas = f\"{bronze_path}cnpj/naturezas\"\n",
    "output_path_cnaes = f\"{bronze_path}cnpj/cnaes\"\n",
    "output_path_motivos = f\"{bronze_path}cnpj/motivos\"\n",
    "\n",
    "print(f\"Input path: {input_path_descompactado}\")\n",
    "print(f\"Output EMPRESAS: {output_path_empresas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1b47301-7e7f-40bd-9b6e-577eacbcea05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão EMPRESAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e590c258-87b6-4185-bcde-8874aee115b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO INGESTÃO EMPRESAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "empresas_columns = [\n",
    "    \"CNPJ_BASICO\",\n",
    "    \"RAZAO_SOCIAL_NOME_EMPRESARIAL\",\n",
    "    \"NATUREZA_JURIDICA\",\n",
    "    \"QUALIFICACAO_DO_RESPONSAVEL\",\n",
    "    \"CAPITAL_SOCIAL_DA_EMPRESA\",\n",
    "    \"PORTE_DA_EMPRESA\",\n",
    "    \"ENTE_FEDERATIVO_RESPONSAVEL\"\n",
    "]\n",
    "\n",
    "schema_empresas = StructType([StructField(c, StringType(), True) for c in empresas_columns])\n",
    "\n",
    "# CORRIGIDO: input_path direto + sem glob\n",
    "df_raw_empresas, df_empresas_corrupt, empresas_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path=\"dbfs:/tmp/deszipados/Empresas/*.EMPRECSV\",  # ← Caminho completo\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=False,  # ← Não recursivo\n",
    "    path_glob_filter=None,  # ← Sem filtro\n",
    "    header=False,\n",
    "    schema=schema_empresas,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/cnpj/empresas\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"EMPRESAS lidos: {df_raw_empresas.count()} registros\")\n",
    "print(f\"EMPRESAS corrompidos: {df_empresas_corrupt.count() if df_empresas_corrupt is not None else 0} registros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdefc82c-10b0-4d53-b569-61d57ab9a711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar incremental APÓS leitura\n",
    "try:\n",
    "    df_processados_empresas = spark.table(\"bronze.arquivos_processados_cnpj\") \\\n",
    "        .filter(col(\"tipo_operacao\") == TIPO_EMPRESAS) \\\n",
    "        .select(\"nome_arquivo\") \\\n",
    "        .distinct()\n",
    "except:\n",
    "    df_processados_empresas = spark.createDataFrame([], StructType([StructField(\"nome_arquivo\", StringType(), True)]))\n",
    "\n",
    "df_novos_empresas = (\n",
    "    df_raw_empresas\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "# Filtro incremental por nome\n",
    "df_novos_empresas = df_novos_empresas.join(\n",
    "    df_processados_empresas,\n",
    "    df_novos_empresas.file_name_only == df_processados_empresas.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"EMPRESAS novos para processar: {df_novos_empresas.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4c2aba-8061-44c3-8415-716e2ff74d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enriquecimento Bronze\n",
    "df_bronze_empresas = (\n",
    "    df_novos_empresas\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    "    .withColumn(\"anomes\", concat(year(col(\"ingestion_dt\")), month(col(\"ingestion_dt\"))))\n",
    ")\n",
    "\n",
    "# Escrita incremental (append)\n",
    "(\n",
    "    df_bronze_empresas\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"anomes\")\n",
    "    .save(output_path_empresas)\n",
    ")\n",
    "\n",
    "print(f\"EMPRESAS escritos em Bronze: {df_bronze_empresas.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65822526-2f30-4c2b-be2e-311866720e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar arquivos processados (apenas os novos)\n",
    "df_registro_empresas = (\n",
    "    df_bronze_empresas\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    "    .select(\n",
    "        col(\"origin_file_name\").alias(\"caminho_arquivo\"),\n",
    "        col(\"file_name_only\").alias(\"nome_arquivo\"),\n",
    "        lit(TIPO_EMPRESAS).alias(\"tipo_operacao\"),\n",
    "        current_timestamp().alias(\"data_processamento\"),\n",
    "        col(\"num_registros\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Escrita na tabela de controle\n",
    "(\n",
    "    df_registro_empresas\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"bronze.arquivos_processados_cnpj\")\n",
    ")\n",
    "\n",
    "print(\"EMPRESAS registrados em controle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec191310-979d-46f5-a68c-651f39fa7bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.arquivos_processados_cnpj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32c9b20f-b386-4694-ac7f-6a01a76ac32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão ESTABELECIMENTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7fca13-a618-47e7-9aa3-4e75e93eed12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO INGESTÃO ESTABELECIMENTOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "estabelecimentos_columns = [\n",
    "    \"CNPJ_BASICO\",\n",
    "    \"CNPJ_ORDEM\",\n",
    "    \"CNPJ_DV\",\n",
    "    \"IDENTIFICADOR_MATRIZ_FILIAL\",\n",
    "    \"NOME_FANTASIA\",\n",
    "    \"SITUACAO_CADASTRAL\",\n",
    "    \"DATA_SITUACAO_CADASTRAL\",\n",
    "    \"MOTIVO_SITUACAO_CADASTRAL\",\n",
    "    \"NOME_CIDADE_EXTERIOR\",\n",
    "    \"PAIS\",\n",
    "    \"DT_INICIO_ATIVIDADE\",\n",
    "    \"CNAE_FISCAL_PRINCIPAL\",\n",
    "    \"CNAE_FISCAL_SECUNDARIA\",\n",
    "    \"TIPO_LOGRADOURO\",\n",
    "    \"LOGRADOURO\",\n",
    "    \"NUMERO\",\n",
    "    \"COMPLEMENTO\",\n",
    "    \"BAIRRO\",\n",
    "    \"CEP\",\n",
    "    \"UF\",\n",
    "    \"MUNICIPIO\",\n",
    "    \"DDD1\",\n",
    "    \"TELEFONE1\",\n",
    "    \"DDD2\",\n",
    "    \"TELEFONE2\",\n",
    "    \"DDD_FAX\",\n",
    "    \"FAX\",\n",
    "    \"CORREIO_ELETRONICO\",\n",
    "    \"SITUACAO_ESPECIAL\",\n",
    "    \"DATA_SITUACAO_ESPECIAL\"\n",
    "]\n",
    "\n",
    "schema_estabelecimentos = StructType([StructField(c, StringType(), True) for c in estabelecimentos_columns])\n",
    "\n",
    "df_raw_estab, df_estab_corrupt, estab_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path=\"dbfs:/tmp/deszipados/Estabelecimentos/*.ESTABELE\",  # ← Caminho completo\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=False,\n",
    "    path_glob_filter=None,\n",
    "    header=False,\n",
    "    schema=schema_estabelecimentos,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/cnpj/estabelecimentos\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"ESTABELECIMENTOS lidos: {df_raw_estab.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb3e610-7627-4f70-846a-5e2dfcb4d64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_processados_estab = spark.table(\"bronze.arquivos_processados_cnpj\") \\\n",
    "        .filter(col(\"tipo_operacao\") == \"ESTABELECIMENTOS\") \\\n",
    "        .select(\"nome_arquivo\") \\\n",
    "        .distinct()\n",
    "except:\n",
    "    df_processados_estab = spark.createDataFrame([], StructType([StructField(\"nome_arquivo\", StringType(), True)]))\n",
    "\n",
    "df_novos_estab = (\n",
    "    df_raw_estab\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "df_novos_estab = df_novos_estab.join(\n",
    "    df_processados_estab,\n",
    "    df_novos_estab.file_name_only == df_processados_estab.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"ESTABELECIMENTOS novos para processar: {df_novos_estab.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e538b4-de8d-45f9-a307-247c1e40d168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enriquecimento Bronze\n",
    "df_bronze_estab = (\n",
    "    df_novos_estab\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    "    .withColumn(\"anomes\", concat(year(col(\"ingestion_dt\")), month(col(\"ingestion_dt\"))))\n",
    ")\n",
    "\n",
    "(\n",
    "    df_bronze_estab\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"anomes\")\n",
    "    .save(f\"{bronze_path}cnpj/estabelecimentos\")\n",
    ")\n",
    "\n",
    "print(f\"ESTABELECIMENTOS escritos em Bronze: {df_bronze_estab.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10295b81-b817-45ae-8aa3-476cdbf68815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar arquivos processados\n",
    "df_registro_estab = (\n",
    "    df_bronze_estab\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    "    .select(\n",
    "        col(\"origin_file_name\").alias(\"caminho_arquivo\"),\n",
    "        col(\"file_name_only\").alias(\"nome_arquivo\"),\n",
    "        lit(\"ESTABELECIMENTOS\").alias(\"tipo_operacao\"),\n",
    "        current_timestamp().alias(\"data_processamento\"),\n",
    "        col(\"num_registros\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    df_registro_estab\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"bronze.arquivos_processados_cnpj\")\n",
    ")\n",
    "\n",
    "print(\"ESTABELECIMENTOS registrados em controle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4256c3fa-15c4-48dc-9fa3-0b24361b0b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão SIMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b49857ea-7b75-4085-bf2a-acb804197ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO INGESTÃO SIMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "simples_columns = [\n",
    "    \"CNPJ_BASICO\",\n",
    "    \"OPCAO_PELO_SIMPLES\",\n",
    "    \"DATA_DE_OPCAO_PELO_SIMPLES\",\n",
    "    \"DATA_DE_EXCLUSAO_DO_SIMPLES\",\n",
    "    \"OPCAO_PELO_MEI\",\n",
    "    \"DATA_DE_OPCAO_PELO_MEI\",\n",
    "    \"DATA_DE_EXCLUSAO_DO_MEI\"\n",
    "]\n",
    "\n",
    "schema_simples = StructType([StructField(c, StringType(), True) for c in simples_columns])\n",
    "\n",
    "df_raw_simples, df_simples_corrupt, simples_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path='dbfs:/tmp/deszipados/Simples',\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=True,\n",
    "    path_glob_filter=None,\n",
    "    header=False,\n",
    "    schema=schema_simples,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/cnpj/simples\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"SIMPLES lidos: {df_raw_simples.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866cbd51-7c3c-45d1-971e-536acd518861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_processados_simples = spark.table(\"bronze.arquivos_processados_cnpj\") \\\n",
    "        .filter(col(\"tipo_operacao\") == TIPO_SIMPLES) \\\n",
    "        .select(\"nome_arquivo\") \\\n",
    "        .distinct()\n",
    "except:\n",
    "    df_processados_simples = spark.createDataFrame([], StructType([StructField(\"nome_arquivo\", StringType(), True)]))\n",
    "\n",
    "df_novos_simples = (\n",
    "    df_raw_simples\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "df_novos_simples = df_novos_simples.join(\n",
    "    df_processados_simples,\n",
    "    df_novos_simples.file_name_only == df_processados_simples.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"SIMPLES novos para processar: {df_novos_simples.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b26c2-a27d-44d0-887e-b3a782e8939a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_simples = (\n",
    "    df_novos_simples\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    ")\n",
    "\n",
    "(\n",
    "    df_bronze_simples\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(output_path_simples)\n",
    ")\n",
    "\n",
    "print(f\"SIMPLES escritos em Bronze: {df_bronze_simples.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6df299-9b9b-4e60-b7ef-efaef15b0ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_registro_simples = (\n",
    "    df_bronze_simples\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    "    .select(\n",
    "        col(\"origin_file_name\").alias(\"caminho_arquivo\"),\n",
    "        col(\"file_name_only\").alias(\"nome_arquivo\"),\n",
    "        lit(TIPO_SIMPLES).alias(\"tipo_operacao\"),\n",
    "        current_timestamp().alias(\"data_processamento\"),\n",
    "        col(\"num_registros\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    df_registro_simples\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"bronze.arquivos_processados_cnpj\")\n",
    ")\n",
    "\n",
    "print(\"SIMPLES registrados em controle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fc78aab-1d0e-4f95-b2ef-60fb6e49c426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão SOCIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19af5ad-a103-4453-8851-432df93ee35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INICIANDO INGESTÃO SOCIOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "socios_columns = [\n",
    "    \"CNPJ_BASICO\",\n",
    "    \"IDENTIFICADOR_DO_SOCIO\",\n",
    "    \"NOME_DO_SOCIO\",\n",
    "    \"CPF_CNPJ_DO_SOCIO\",\n",
    "    \"QUALIFICACAO_DO_SOCIO\",\n",
    "    \"DATA_DE_ENTRADA_NA_SOCIEDADE\",\n",
    "    \"PAIS\",\n",
    "    \"REPRESENTANTE_LEGAL\",\n",
    "    \"NOME_DO_REPRESENTANTE_LEGAL\",\n",
    "    \"QUALIFICACAO_DO_REPRESENTANTE_LEGAL\",\n",
    "    \"FAIXA_ETARIA\"\n",
    "]\n",
    "\n",
    "schema_socios = StructType([StructField(c, StringType(), True) for c in socios_columns])\n",
    "\n",
    "df_raw_socios, df_socios_corrupt, socios_cols = read_csv_with_quotes(\n",
    "    spark=spark,\n",
    "    input_path='dbfs:/tmp/deszipados/Socios',\n",
    "    delimiter=\";\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    recursive=True,\n",
    "    path_glob_filter=None,\n",
    "    header=False,\n",
    "    schema=schema_socios,\n",
    "    expected_cols=None,\n",
    "    multiline=True,\n",
    "    quote=\"\\\"\",\n",
    "    escape=\"\\\"\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    corrupt_col=\"_corrupt_record\",\n",
    "    ignore_leading_trailing_ws=True,\n",
    "    quarantine_path=\"/mnt/bronze_quarentena/cnpj/socios\",\n",
    "    quarantine_mode=\"append\"\n",
    ")\n",
    "\n",
    "print(f\"✔ SOCIOS lidos: {df_raw_socios.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d88a6b-177a-4a30-a353-9fb8d3106785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_processados_socios = spark.table(\"bronze.arquivos_processados_cnpj\") \\\n",
    "        .filter(col(\"tipo_operacao\") == TIPO_SOCIOS) \\\n",
    "        .select(\"nome_arquivo\") \\\n",
    "        .distinct()\n",
    "except:\n",
    "    df_processados_socios = spark.createDataFrame([], StructType([StructField(\"nome_arquivo\", StringType(), True)]))\n",
    "\n",
    "df_novos_socios = (\n",
    "    df_raw_socios\n",
    "    .withColumn(\"origin_file_name\", input_file_name())\n",
    "    .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    ")\n",
    "\n",
    "df_novos_socios = df_novos_socios.join(\n",
    "    df_processados_socios,\n",
    "    df_novos_socios.file_name_only == df_processados_socios.nome_arquivo,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(f\"SOCIOS novos para processar: {df_novos_socios.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c754d94-444e-4f4d-82ed-cb933fc3e863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_socios = (\n",
    "    df_novos_socios\n",
    "    .withColumn(\"ingestion_dt\", current_timestamp())\n",
    ")\n",
    "\n",
    "(\n",
    "    df_bronze_socios\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(output_path_socios)\n",
    ")\n",
    "\n",
    "print(f\"SOCIOS escritos em Bronze: {df_bronze_socios.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fa1245-47d2-43aa-ac58-36aa3509bc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_registro_socios = (\n",
    "    df_bronze_socios\n",
    "    .groupBy(\"origin_file_name\", \"file_name_only\")\n",
    "    .agg(count(\"*\").alias(\"num_registros\"))\n",
    "    .select(\n",
    "        col(\"origin_file_name\").alias(\"caminho_arquivo\"),\n",
    "        col(\"file_name_only\").alias(\"nome_arquivo\"),\n",
    "        lit(TIPO_SOCIOS).alias(\"tipo_operacao\"),\n",
    "        current_timestamp().alias(\"data_processamento\"),\n",
    "        col(\"num_registros\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    df_registro_socios\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"bronze.arquivos_processados_cnpj\")\n",
    ")\n",
    "\n",
    "print(\"SOCIOS registrados em controle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e04ee7-139d-41b7-bedc-7b9d777c8768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestão TABELAS AUXILIARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05dad03b-3329-4ea7-87da-3415488f1291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabelas_auxiliares = [\n",
    "    (\"Paises\", \"PAISES\", output_path_paises, \"*\"),\n",
    "    (\"Municipios\", \"MUNICIPIOS\", output_path_municipios, \"*\"),\n",
    "    (\"Qualificacoes\", \"QUALIFICACOES\", output_path_qualificacoes, \"*\"),\n",
    "    (\"Naturezas\", \"NATUREZAS\", output_path_naturezas, \"*\"),\n",
    "    (\"Cnaes\", \"CNAES\", output_path_cnaes, \"*\"),\n",
    "    (\"Motivos\", \"MOTIVOS\", output_path_motivos, \"*\")\n",
    "]\n",
    "\n",
    "aux_columns = [\"CODIGO\", \"DESCRICAO\"]\n",
    "schema_aux = StructType([StructField(c, StringType(), True) for c in aux_columns])\n",
    "\n",
    "for pasta, tipo_operacao, output_path, extensao in tabelas_auxiliares:\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"INICIANDO INGESTÃO {tipo_operacao}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # CORRIGIDO: Usar caminho completo direto (Teste 1)\n",
    "        input_path_completo = f\"dbfs:/tmp/deszipados/{pasta}/{extensao}\"\n",
    "        \n",
    "        print(f\"Lendo de: {input_path_completo}\")\n",
    "        \n",
    "        # Leitura com read_csv_with_quotes\n",
    "        df_raw_aux, df_aux_corrupt, aux_cols = read_csv_with_quotes(\n",
    "            spark=spark,\n",
    "            input_path=input_path_completo,  # ← Caminho completo\n",
    "            delimiter=\";\",\n",
    "            encoding=\"iso-8859-1\",\n",
    "            recursive=False,  # ← Não recursivo\n",
    "            path_glob_filter=None,  # ← Sem filtro\n",
    "            header=False,\n",
    "            schema=schema_aux,\n",
    "            expected_cols=None,\n",
    "            multiline=True,\n",
    "            quote=\"\\\"\",\n",
    "            escape=\"\\\"\",\n",
    "            mode=\"PERMISSIVE\",\n",
    "            corrupt_col=\"_corrupt_record\",\n",
    "            ignore_leading_trailing_ws=True,\n",
    "            quarantine_path=f\"/mnt/bronze_quarentena/cnpj/{pasta.lower()}\",\n",
    "            quarantine_mode=\"append\"\n",
    "        )\n",
    "        \n",
    "        print(f\"{tipo_operacao} lidos: {df_raw_aux.count()} registros\")\n",
    "        \n",
    "        # Aplicar incremental\n",
    "        try:\n",
    "            df_processados_aux = spark.table(\"bronze.arquivos_processados_cnpj\") \\\n",
    "                .filter(col(\"tipo_operacao\") == tipo_operacao) \\\n",
    "                .select(\"nome_arquivo\") \\\n",
    "                .distinct()\n",
    "        except:\n",
    "            df_processados_aux = spark.createDataFrame([], StructType([StructField(\"nome_arquivo\", StringType(), True)]))\n",
    "        \n",
    "        df_novos_aux = (\n",
    "            df_raw_aux\n",
    "            .withColumn(\"origin_file_name\", input_file_name())\n",
    "            .withColumn(\"file_name_only\", regexp_extract(col(\"origin_file_name\"), \"[^/]+$\", 0))\n",
    "        )\n",
    "        \n",
    "        df_novos_aux = df_novos_aux.join(\n",
    "            df_processados_aux,\n",
    "            df_novos_aux.file_name_only == df_processados_aux.nome_arquivo,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        print(f\"{tipo_operacao} novos para processar: {df_novos_aux.count()}\")\n",
    "        \n",
    "        # Enriquecimento\n",
    "        df_bronze_aux = (\n",
    "            df_novos_aux\n",
    "            .withColumn(\"ingestion_dt\", current_timestamp())\n",
    "        )\n",
    "        \n",
    "        # Escrita\n",
    "        (\n",
    "            df_bronze_aux\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .save(output_path)\n",
    "        )\n",
    "        \n",
    "        print(f\"✔ {tipo_operacao} escritos em Bronze: {df_bronze_aux.count()} registros\")\n",
    "        \n",
    "        # Registrar\n",
    "        df_registro_aux = (\n",
    "            df_bronze_aux\n",
    "            .groupBy(\"origin_file_name\", \"file_name_only\")\n",
    "            .agg(count(\"*\").alias(\"num_registros\"))\n",
    "            .select(\n",
    "                col(\"origin_file_name\").alias(\"caminho_arquivo\"),\n",
    "                col(\"file_name_only\").alias(\"nome_arquivo\"),\n",
    "                lit(tipo_operacao).alias(\"tipo_operacao\"),\n",
    "                current_timestamp().alias(\"data_processamento\"),\n",
    "                col(\"num_registros\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        (\n",
    "            df_registro_aux\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(\"bronze.arquivos_processados_cnpj\")\n",
    "        )\n",
    "        \n",
    "        print(f\"{tipo_operacao} registrados em controle\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao processar {tipo_operacao}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d8d1a2-63cd-4797-ac5f-f100c1049cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resumo da ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad341072-976a-453b-ab04-44597a5b924c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESUMO DA INGESTÃO INCREMENTAL CNPJ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_resumo = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        tipo_operacao,\n",
    "        COUNT(*) as total_arquivos,\n",
    "        SUM(num_registros) as total_registros,\n",
    "        MAX(data_processamento) as ultima_ingestao\n",
    "    FROM bronze.arquivos_processados_cnpj\n",
    "    GROUP BY tipo_operacao\n",
    "    ORDER BY tipo_operacao\n",
    "\"\"\")\n",
    "\n",
    "df_resumo.display()\n",
    "\n",
    "print(\"\\n Ingestão CNPJ finalizada com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8739614389901858,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_incremental_cnpj",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
