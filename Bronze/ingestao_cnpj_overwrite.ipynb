{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48092920-e679-4360-8edd-f08bf107de81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importação de configurações e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a55afa99-6dfe-4e2a-8689-3bfd98b92e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import zipfile\n",
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import lit\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5955ce3d-d239-4e78-b718-7f437ed9f8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Users/kgenuins@emeal.nttdata.com/project-insight-lab-databricks\")\n",
    "\n",
    "from Config.spark_config import apply_storage_config\n",
    "from Config.storage_config import *\n",
    "\n",
    "apply_storage_config(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b595bf75-6224-404c-a3df-595b321179a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_path = f\"{cnpj_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d171784c-534f-4af5-8890-2c8fc365eedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Aquisição de dados(Camada Bronze)\n",
    "\n",
    "* Os dados originalmente são depositados no \"blob\" da Azure\n",
    "\n",
    "* Então, os arquivos são lidos do \"blob\" e descompactados diretamente no cluster Databricks em um diretório abaixo do /tmp do cluster\n",
    "\n",
    "* Os dados relacionados à CNPJ relevantes no momento são:\n",
    "  * Empresas\n",
    "  * Estabelecimentos\n",
    "  * Simples\n",
    "  * Sócios\n",
    "  * Países\n",
    "  * Municípios\n",
    "  * Qualificações\n",
    "  * Naturezas\n",
    "  * CNAEs\n",
    "\n",
    "Para a camada \"Bronze\", apenas os headers são adicionados além das colunas da data de ingestão e do path de origem dos dados\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c594456-3d05-40c5-b965-f222baccf3c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arquivos = dbutils.fs.ls(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb34859c-8e08-4e1f-ab3c-5257408df662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Descomprimindo os arquivos zip\n",
    "input_path = f\"{cnpj_path}\"  \n",
    "# Path temporário para os arquivos descompactados\n",
    "output_path = \"/dbfs/tmp/deszipados\"  # pasta de destino (DBFS)\n",
    "\n",
    "# Cria a pasta de destino se não existir\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Lista arquivos na pasta do DBFS\n",
    "arquivos = dbutils.fs.ls(input_path)\n",
    "\n",
    "def decompress_file(arquivo):\n",
    "    \"\"\"Descompassa um arquivo zip no DBFS.\n",
    "    \"\"\"\n",
    "    if arquivo.name.endswith(\".zip\"):\n",
    "        # Caminho do zip no DBFS\n",
    "        caminho_zip_dbfs = arquivo.path\n",
    "\n",
    "        # Caminho local temporário para descompactar\n",
    "        caminho_zip_local = f\"/tmp/{arquivo.name}\"\n",
    "\n",
    "        # Copia do DBFS para local\n",
    "        dbutils.fs.cp(caminho_zip_dbfs, f\"file:{caminho_zip_local}\")\n",
    "\n",
    "        # Cria uma subpasta para cada zip\n",
    "        nome_subpasta = os.path.splitext(arquivo.name)[0]\n",
    "        caminho_subpasta = os.path.join(output_path, nome_subpasta)\n",
    "        os.makedirs(caminho_subpasta, exist_ok=True)\n",
    "\n",
    "        # Descompacta\n",
    "        with zipfile.ZipFile(caminho_zip_local, 'r') as zip_ref:\n",
    "            zip_ref.extractall(caminho_subpasta)\n",
    "        \n",
    "        print(f\"Descompactado {arquivo.name} em {caminho_subpasta}\")\n",
    "\n",
    "# Descomprime cada arquivo da lista 'arquivos'\n",
    "[decompress_file(arquivo) for arquivo in arquivos]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15856c1-61bd-4601-a715-4a5b07e9e266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Listando os arquivos deszipados. Não relevante para o pipeline. Usar somente para depuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6edf3d05-33bf-4943-8688-323d887ddc82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(cnpj_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc90123d-3864-4cfd-aec5-902497b28f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a6eb32-7b49-409b-be49-6d753e4b11c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def default_treatment_bronze_df(df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Tratamento padrão para DataFrames na camada Bronze. Cria uma coluna para gravar o caminho de origem dos dados e a data de ingestão.\n",
    "    \n",
    "    df: DataFrame a ser tratado.\n",
    "    return: DataFrame tratado.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Adicicionando a coluna ingestion_dt \n",
    "    df = df.withColumn(\"ingestion_dt\", current_timestamp())\n",
    "    # Criando uma coluna de particionamento chamada 'anomes', cujo conteúdo é extraido das informações 'ano' e 'mês' da coluna ingestion_dt\n",
    "    df = df.withColumn(\"anomes\", concat(year(df.ingestion_dt), month(df.ingestion_dt)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfeda25-1881-4b9c-81b9-af03bc405b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de empresas\n",
    "# Coluna dos arquivos de EMPRESAS\n",
    "# TODO colocar a definição do esquema em um lugar mais apropriado a definir.\n",
    "empresas_columns = [\n",
    "    \"CNPJ_BASICO\", \n",
    "    \"RAZAO_SOCIAL_NOME_EMPRESARIAL\", \n",
    "    \"NATUREZA_JURIDICA\", \n",
    "    \"QUALIFICACAO_DO_RESPONSAVEL\",  \n",
    "    \"CAPITAL_SOCIAL_DA_EMPRESA\",\n",
    "    \"PORTE_DA_EMPRESA\",\n",
    "    \"ENTE_FEDERATIVO_RESPONSAVEL\"\n",
    "]\n",
    "try:\n",
    "    # Caminho de acesso para todos os arquivos descompactados de empresas\n",
    "    companies_path = \"dbfs:/tmp/deszipados/Empresas*/*.EMPRECSV\"\n",
    "    # Lendo e unificando os arquivos descompactados Empresas0,1,2,3...\n",
    "    empresas_df = spark.read.csv(\n",
    "        companies_path, \n",
    "        header=False, \n",
    "        encoding=\"iso-8859-1\", sep=';'\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: Não foi possível ler os dados de EMPRESAS no caminho '{companies_path}' - {e}\")\n",
    "    # Abortando a execução\n",
    "    dbutils.notebook.exit()\n",
    "    \n",
    "# Iniciando a ingestão de dados de empresas\n",
    "try:\n",
    "    # Adicionando o 'header' com os nomes das colunas\n",
    "    empresas_df = empresas_df.toDF(*empresas_columns)\n",
    "    # Criando uma coluna para gravar o caminho de origem dos dados\n",
    "    empresas_df = empresas_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Empresas*/*.EMPRECSV\"))\n",
    "    # Tratamento padrão para as tabelas bronze\n",
    "    empresas_df = default_treatment_bronze_df(empresas_df)\n",
    "    # print(empresas_df.schema)\n",
    "    display(empresas_df)\n",
    "\n",
    "    # Gravando a Delta em modo 'overwrite' na camada \"Bronze\"\n",
    "    (empresas_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"anomes\")\n",
    "        .save(f\"{bronze_path}/cnpj/Empresas\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: A ingestão dos dados de empresa em {bronze_path}/cnpj/Empresas. falhou! - {e}\")\n",
    "    dbutils.notebook.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7eed35-19d3-4376-ae50-00eea43087fc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770227973253}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estabelecimentos\n",
    "estabelecimentos_columns = [\n",
    "    \"CNPJ_BASICO\",\n",
    "    \"CNPJ_ORDEM\",\n",
    "    \"CNPJ_DV\",\n",
    "    \"IDENTIFICADOR_MATRIZ_FILIAL\",\n",
    "    \"NOME_FANTASIA\",\n",
    "    \"SITUACAO_CADASTRAL\",\n",
    "    \"DATA_SITUACAO_CADASTRAL\",\n",
    "    \"MOTIVO_SITUACAO_CADASTRAL\",\n",
    "    \"NOME_CIDADE_EXTERIOR\",\n",
    "    \"PAIS\",\n",
    "    \"DT_INICIO_ATIVIDADE\",\n",
    "    \"CNAE_FISCAL_PRINCIPAL\",\n",
    "    \"CNAE_FISCAL_SECUNDARIA\",\n",
    "    \"TIPO_LOGRADOURO\",\n",
    "    \"LOGRADOURO\",\n",
    "    \"NUMERO\",\n",
    "    \"COMPLEMENTO\",\n",
    "    \"BAIRRO\",\n",
    "    \"CEP\",\n",
    "    \"UF\",\n",
    "    \"MUNICIPIO\",\n",
    "    \"DDD1\",\n",
    "    \"TELEFONE1\",\n",
    "    \"DDD2\",\n",
    "    \"TELEFONE2\",\n",
    "    \"DDD_FAX\",\n",
    "    \"FAX\",\n",
    "    \"CORREIO_ELETRONICO\",\n",
    "    \"SITUACAO_ESPECIAL\",\n",
    "    \"DATA_SITUACAO_ESPECIAL\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    estabelecimentos_df = spark.read.csv(\"dbfs:/tmp/deszipados/Estabelecimentos*/*.ESTABELE\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "    # Adicionando o 'header' com os nomes das colunas\n",
    "    estabelecimentos_df = estabelecimentos_df.toDF(*estabelecimentos_columns)\n",
    "    # Criando uma coluna para gravar o caminho de origem dos dados\n",
    "    estabelecimentos_df = estabelecimentos_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Estabelecimentos*/*.ESTABELE\"))\n",
    "    # Tratamento padrão para as tabelas bronze\n",
    "    estabelecimentos_df = default_treatment_bronze_df(estabelecimentos_df)\n",
    "    display(estabelecimentos_df)\n",
    "    # Gravando na camada bronze usando delta\n",
    "    ( \n",
    "    estabelecimentos_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"anomes\")\n",
    "        .save(f\"{bronze_path}/cnpj/Estabelecimentos\")\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: A ingestão dos dados de estabelecimentos em {bronze_path}/cnpj/Estabelecimentos falhou! - {e}\")\n",
    "    dbutils.notebook.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ede5f70-c165-49d2-b605-9ceb2bb72cf4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770231827937}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de simples\n",
    "# Coluna dos arquivos de simples\n",
    "simples_columns = [\n",
    "    \"CNPJ_BASICO\", \n",
    "    \"OPCAO_PELO_SIMPLES\", \n",
    "    \"DATA_DE_OPCAO_PELO_SIMPLES\", \n",
    "    \"DATA_DE_EXCLUSAO_DO_SIMPLES\", \n",
    "    \"OPCAO_PELO_MEI\", \n",
    "    \"DATA_DE_OPCAO_PELO_MEI\", \n",
    "    \"DATA_DE_EXCLUSAO_DO_MEI\"\n",
    "]\n",
    "try:\n",
    "    # Lendo e unificando os arquivos \n",
    "    simples_df = spark.read.csv(\"dbfs:/tmp/deszipados/Simples*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "    simples_df = simples_df.toDF(*simples_columns)\n",
    "\n",
    "    # Criando uma coluna para gravar o caminho de origem dos dados\n",
    "    simples_df = simples_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Simples*\"))\n",
    "\n",
    "    # Tratamento padrão para as tabelas bronze\n",
    "    simples_df = default_treatment_bronze_df(simples_df)\n",
    "\n",
    "    display(simples_df)\n",
    "    # Gravando os dados na camada bronze\n",
    "    (\n",
    "        simples_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(f\"{bronze_path}/cnpj/Simples\")    \n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: A ingestão dos dados de simples em {bronze_path}/cnpj/Simples falhou! - {e}\")\n",
    "    dbutils.notebook.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02689a2-0838-4444-bc5a-2dc232af051c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770231790401}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Arquivo de sócios \n",
    "\n",
    "# Padronizando as colunas dos arquivos de sócios\n",
    "socios_columns = [\n",
    "    \"CNPJ_BASICO\", \n",
    "    \"IDENTIFICADOR_DO_SOCIO\", \n",
    "    \"NOME_DO_SOCIO\", \n",
    "    \"CPF/CNPJ_DO_SOCIO\", \n",
    "    \"QUALIFICACAO_DO_SOCIO\", \n",
    "    \"DATA_DE_ENTRADA_NA_SOCIEDADE\", \n",
    "    \"PAIS\", \n",
    "    \"REPRESENTANTE_LEGAL\", \n",
    "    \"NOME_DO_REPRESENTANTE_LEGAL\", \n",
    "    \"QUALIFICACAO_DO_REPRESENTANTE_LEGAL\",\n",
    "    \"FAIXA_ETARIA\"\n",
    "]\n",
    "\n",
    "# Leitura dos arquivos CSV\n",
    "socios_df = spark.read.csv(\n",
    "    \"dbfs:/tmp/deszipados/Socios*/*.SOCIOCSV\",\n",
    "    header=False,\n",
    "    sep=\";\",\n",
    "    encoding=\"iso-8859-1\"\n",
    ")\n",
    "\n",
    "socios_df = socios_df.toDF(*socios_columns)\n",
    "\n",
    "# Adicionando colunas de data de ingestão\n",
    "socios_df = default_treatment_bronze_df(socios_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "socios_df = socios_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Socios*/*.SOCIOCSV\"))\n",
    "\n",
    "#Gravando em delta.\n",
    "try:\n",
    "    (\n",
    "        socios_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Socios\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "\n",
    "display(socios_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68018f02-76ad-43e4-93ce-7311af9cd522",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770232958310}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de Países\n",
    "\n",
    "# Padronizando as colunas dos arquivos de paises\n",
    "paises_columns = [\n",
    "    \"CODIGO\",\n",
    "    \"DESCRICAO\"\n",
    "]\n",
    "\n",
    "# Leitura dos arquivos CSV\n",
    "paises_df = spark.read.csv(\"dbfs:/tmp/deszipados/Paises*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "\n",
    "paises_df = paises_df.toDF(*paises_columns)\n",
    "\n",
    "# Adicionando colunas de data de ingestão\n",
    "paises_df = default_treatment_bronze_df(paises_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "paises_df = paises_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Paises*\"))\n",
    "\n",
    "# Gravando em delta\n",
    "try:\n",
    "    (\n",
    "        paises_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Paises\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "display(paises_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e21308d-9034-4eea-a4bd-152facd407a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770233001408}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de Municipios\n",
    "\n",
    "# Coluna dos arquivos de Municipios\n",
    "municipios_columns = [\n",
    "    \"CODIGO\",\n",
    "    \"DESCRICAO\"\n",
    "]\n",
    "\n",
    "# Lendo e unificando os arquivos \n",
    "municipios_df = spark.read.csv(\"dbfs:/tmp/deszipados/Municipios*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "\n",
    "municipios_df = municipios_df.toDF(*municipios_columns)\n",
    "\n",
    "# Adicionando as colunas de data de ingestão\n",
    "municipios_df = default_treatment_bronze_df(municipios_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "municipios_df = municipios_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Municipios*\"))\n",
    "\n",
    "# Gravando em delta.\n",
    "try:\n",
    "    (\n",
    "        municipios_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Municipios\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "display(municipios_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acafe7e7-7bd1-4391-b126-5fb57c93c2c3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770232050590}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de Qualificações de empresas\n",
    "\n",
    "# Coluna dos arquivos de Qualificações de empresas\n",
    "qualificacoes_columns = [\n",
    "    \"CODIGO\",\n",
    "    \"DESCRICAO\"\n",
    "]\n",
    "\n",
    "# Lendo e unificando os arquivos \n",
    "qualificacoes_df = spark.read.csv(\"dbfs:/tmp/deszipados/Qualificacoes*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "\n",
    "qualificacoes_df = qualificacoes_df.toDF(*qualificacoes_columns)\n",
    "\n",
    "#Adicionando as colunas de data de ingestão\n",
    "qualificacoes_df = default_treatment_bronze_df(qualificacoes_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "qualificacoes_df = qualificacoes_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Qualificacoes*\"))\n",
    "\n",
    "#Gravando em delta.\n",
    "try:\n",
    "    (\n",
    "        qualificacoes_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Qualificacoes\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "display(qualificacoes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376b3ef1-5155-455b-aa71-9a1bba250fb1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770232080908}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de Naturezas\n",
    "\n",
    "# Coluna dos arquivos de Naturezas\n",
    "naturezas_columns = [\n",
    "    \"CODIGO\",\n",
    "    \"DESCRICAO\"\n",
    "]\n",
    "\n",
    "# Lendo e unificando os arquivos \n",
    "naturezas_df = spark.read.csv(\"dbfs:/tmp/deszipados/Naturezas*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "\n",
    "naturezas_df = naturezas_df.toDF(*naturezas_columns)\n",
    "\n",
    "#Adicionando as colunas de data de ingestão\n",
    "naturezas_df = default_treatment_bronze_df(naturezas_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "naturezas_df = naturezas_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Naturezas*\"))\n",
    "\n",
    "#Gravando em delta.\n",
    "try:\n",
    "    (\n",
    "        naturezas_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Naturezas\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "display(naturezas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee132d5-9188-4639-8aac-ed548139b7cb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770232139996}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de Cnaes\n",
    "\n",
    "# Coluna dos arquivos de Cnaes\n",
    "cnaes_columns = [\n",
    "    \"CODIGO\",\n",
    "    \"DESCRICAO\"\n",
    "]\n",
    "\n",
    "# Lendo e unificando os arquivos \n",
    "cnaes_df = spark.read.csv(\"dbfs:/tmp/deszipados/Cnaes*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "cnaes_df = cnaes_df.toDF(*cnaes_columns)\n",
    "\n",
    "# Adicionando as colunas de data de ingestão\n",
    "cnaes_df = default_treatment_bronze_df(cnaes_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "cnaes_df = cnaes_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Cnaes*\"))\n",
    "\n",
    "#Gravando em delta.\n",
    "try:\n",
    "    (\n",
    "        cnaes_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Cnaes\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "display(cnaes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7803d027-21a9-409e-9492-4be16f81949e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquivos de Motivos\n",
    "\n",
    "# Coluna dos arquivos de Motivos\n",
    "motivos_columns = [\n",
    "    \"CODIGO\",\n",
    "    \"DESCRICAO\"\n",
    "]\n",
    "\n",
    "# Lendo e unificando os arquivos \n",
    "motivos_df = spark.read.csv(\"dbfs:/tmp/deszipados/Motivos*\", header=False, encoding=\"iso-8859-1\", sep=';')\n",
    "motivos_df = motivos_df.toDF(*motivos_columns)\n",
    "\n",
    "# Adicionando as colunas de data de ingestão\n",
    "motivos_df = default_treatment_bronze_df(motivos_df)\n",
    "\n",
    "# Criando uma coluna para gravar o caminho de origem dos dados\n",
    "motivos_df = motivos_df.withColumn(\"origin_path_name\", lit(\"dbfs:/tmp/deszipados/Motivos*\"))\n",
    "\n",
    "#Gravando em delta.\n",
    "try:\n",
    "    (\n",
    "        motivos_df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(f\"{bronze_path}/cnpj/Motivos\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar Delta na camada Bronze\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "display(motivos_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027a77bd-0555-4a71-a35a-06bdb84a0163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run /Workspace/Users/kgenuins@emeal.nttdata.com/project-insight-lab-databricks/Bronze/ingestao_incremental_cnpj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b12ae39-3cc2-4f8a-b8a8-565fa8291389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#processar_csv_bronze_incremental_naturezas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fbcf5c-9744-4904-ab58-d384f48d60de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#naturezas_df, qtd = processar_csv_bronze_incremental_naturezas(\n",
    "#    input_path=\"dbfs:/tmp/deszipados/Naturezas*\",\n",
    "#    output_path=f\"{bronze_path}/cnpj/Naturezas\",\n",
    "#    colunas=[\"CODIGO\", \"DESCRICAO\"]\n",
    "\n",
    "print(f\"Foram inseridos {qtd} registros novos\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5774932292367687,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_cnpj_overwrite",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
